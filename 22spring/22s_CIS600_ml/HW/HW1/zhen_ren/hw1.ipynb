{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "insured-anthropology",
   "metadata": {},
   "source": [
    "Data Prepared\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlled-jerusalem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>70,692 rows × 22 columns (omitted printing of 14 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>Diabetes_binary</th><th>HighBP</th><th>HighChol</th><th>CholCheck</th><th>BMI</th><th>Smoker</th><th>Stroke</th><th>HeartDiseaseorAttack</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>25.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><th>2</th><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>25.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><th>3</th><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>29.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><th>4</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>26.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><th>5</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>29.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><th>6</th><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>24.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>7</th><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>36.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>8</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>26.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><th>9</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>21.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>10</th><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>23.0</td><td>1.0</td><td>0.0</td><td>1.0</td></tr><tr><th>11</th><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>27.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>12</th><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>24.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>13</th><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>25.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><th>14</th><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>30.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>15</th><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>31.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><th>16</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>24.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>17</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>26.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>18</th><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>26.0</td><td>1.0</td><td>0.0</td><td>1.0</td></tr><tr><th>19</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>35.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>20</th><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>35.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><th>21</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>31.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>22</th><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>33.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><th>23</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>25.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>24</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>21.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>25</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>37.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><th>26</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>23.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>27</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>22.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>28</th><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>29.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>29</th><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>26.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>30</th><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>32.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& Diabetes\\_binary & HighBP & HighChol & CholCheck & BMI & Smoker & Stroke & HeartDiseaseorAttack & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.0 & 0.0 & 1.0 & 1.0 & 25.0 & 1.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t2 & 0.0 & 0.0 & 0.0 & 0.0 & 25.0 & 1.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t3 & 0.0 & 1.0 & 1.0 & 1.0 & 29.0 & 1.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t4 & 0.0 & 0.0 & 0.0 & 1.0 & 26.0 & 1.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t5 & 0.0 & 0.0 & 0.0 & 1.0 & 29.0 & 1.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t6 & 0.0 & 0.0 & 0.0 & 0.0 & 24.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t7 & 0.0 & 1.0 & 1.0 & 1.0 & 36.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t8 & 0.0 & 0.0 & 0.0 & 1.0 & 26.0 & 1.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t9 & 0.0 & 0.0 & 0.0 & 1.0 & 21.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t10 & 0.0 & 1.0 & 1.0 & 1.0 & 23.0 & 1.0 & 0.0 & 1.0 & $\\dots$ \\\\\n",
       "\t11 & 0.0 & 1.0 & 1.0 & 1.0 & 27.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t12 & 0.0 & 0.0 & 0.0 & 0.0 & 24.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t13 & 0.0 & 1.0 & 1.0 & 1.0 & 25.0 & 1.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t14 & 0.0 & 1.0 & 1.0 & 1.0 & 30.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t15 & 0.0 & 1.0 & 1.0 & 1.0 & 31.0 & 1.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t16 & 0.0 & 0.0 & 0.0 & 1.0 & 24.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t17 & 0.0 & 0.0 & 0.0 & 1.0 & 26.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t18 & 0.0 & 1.0 & 1.0 & 1.0 & 26.0 & 1.0 & 0.0 & 1.0 & $\\dots$ \\\\\n",
       "\t19 & 0.0 & 0.0 & 0.0 & 1.0 & 35.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t20 & 0.0 & 1.0 & 0.0 & 1.0 & 35.0 & 1.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t21 & 0.0 & 0.0 & 0.0 & 1.0 & 31.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t22 & 0.0 & 1.0 & 1.0 & 1.0 & 33.0 & 1.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t23 & 0.0 & 0.0 & 0.0 & 1.0 & 25.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t24 & 0.0 & 0.0 & 0.0 & 1.0 & 21.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t25 & 0.0 & 0.0 & 0.0 & 1.0 & 37.0 & 1.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t26 & 0.0 & 0.0 & 0.0 & 1.0 & 23.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t27 & 0.0 & 0.0 & 0.0 & 1.0 & 22.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t28 & 0.0 & 0.0 & 1.0 & 1.0 & 29.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t29 & 0.0 & 0.0 & 0.0 & 1.0 & 26.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t30 & 0.0 & 1.0 & 1.0 & 1.0 & 32.0 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m70692×22 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m Diabetes_binary \u001b[0m\u001b[1m HighBP  \u001b[0m\u001b[1m HighChol \u001b[0m\u001b[1m CholCheck \u001b[0m\u001b[1m BMI     \u001b[0m\u001b[1m Smoker  \u001b[0m\u001b[1m Stro\u001b[0m ⋯\n",
       "\u001b[1m       \u001b[0m│\u001b[90m Float64         \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Floa\u001b[0m ⋯\n",
       "───────┼────────────────────────────────────────────────────────────────────────\n",
       "     1 │             0.0      0.0       1.0        1.0     25.0      1.0       ⋯\n",
       "     2 │             0.0      0.0       0.0        0.0     25.0      1.0\n",
       "     3 │             0.0      1.0       1.0        1.0     29.0      1.0\n",
       "     4 │             0.0      0.0       0.0        1.0     26.0      1.0\n",
       "     5 │             0.0      0.0       0.0        1.0     29.0      1.0       ⋯\n",
       "     6 │             0.0      0.0       0.0        0.0     24.0      0.0\n",
       "     7 │             0.0      1.0       1.0        1.0     36.0      0.0\n",
       "     8 │             0.0      0.0       0.0        1.0     26.0      1.0\n",
       "     9 │             0.0      0.0       0.0        1.0     21.0      0.0       ⋯\n",
       "    10 │             0.0      1.0       1.0        1.0     23.0      1.0\n",
       "    11 │             0.0      1.0       1.0        1.0     27.0      0.0\n",
       "   ⋮   │        ⋮            ⋮        ⋮          ⋮         ⋮        ⋮        ⋮ ⋱\n",
       " 70683 │             1.0      1.0       0.0        1.0     37.0      0.0\n",
       " 70684 │             1.0      1.0       0.0        1.0     28.0      0.0       ⋯\n",
       " 70685 │             1.0      1.0       1.0        1.0     27.0      0.0\n",
       " 70686 │             1.0      1.0       0.0        1.0     38.0      0.0\n",
       " 70687 │             1.0      0.0       1.0        1.0     27.0      0.0\n",
       " 70688 │             1.0      0.0       1.0        1.0     37.0      0.0       ⋯\n",
       " 70689 │             1.0      0.0       1.0        1.0     29.0      1.0\n",
       " 70690 │             1.0      1.0       1.0        1.0     25.0      0.0\n",
       " 70691 │             1.0      1.0       1.0        1.0     18.0      0.0\n",
       " 70692 │             1.0      1.0       1.0        1.0     25.0      0.0       ⋯\n",
       "\u001b[36m                                               16 columns and 70671 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ZipFile, CSV, DataFrames, Random, StatsBase , Plots, Statistics\n",
    "\n",
    "# read file from zip archive\n",
    "\n",
    "z = ZipFile.Reader(\"results.zip\")\n",
    "\n",
    "# identify the right file in zip\n",
    "\n",
    "# The diabetes dataset I found through kaggle have done split into 2 classifiers and select 50 percents of result from each \n",
    "# label. So i think there is more work in my dataset\n",
    "\n",
    "a_file_in_zip = filter(x->x.name == \"diabetes_binary_5050split_health_indicators_BRFSS2015.csv\", z.files)[1]\n",
    "\n",
    "#avoid changing the original files in the zip file. However, the dataset will not change but whatever.\n",
    "\n",
    "a_copy = CSV.File(a_file_in_zip) |> DataFrame\n",
    "\n",
    "close(z)\n",
    "\n",
    "#show the dataset\n",
    "\n",
    "a_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "postal-class",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35346×22 Matrix{Float64}:\n",
       " 1.0  1.0  1.0  1.0  30.0  1.0  0.0  1.0  …  30.0  1.0  0.0   9.0  5.0  1.0\n",
       " 1.0  0.0  0.0  1.0  25.0  1.0  0.0  0.0      0.0  0.0  1.0  13.0  6.0  8.0\n",
       " 1.0  1.0  1.0  1.0  28.0  0.0  0.0  0.0      0.0  1.0  0.0  11.0  4.0  6.0\n",
       " 1.0  0.0  0.0  1.0  23.0  1.0  0.0  0.0      0.0  0.0  1.0   7.0  5.0  6.0\n",
       " 1.0  1.0  0.0  1.0  27.0  0.0  0.0  0.0      0.0  0.0  0.0  13.0  5.0  4.0\n",
       " 1.0  1.0  1.0  1.0  37.0  1.0  1.0  1.0  …   0.0  1.0  1.0  10.0  6.0  5.0\n",
       " 1.0  1.0  1.0  1.0  28.0  1.0  0.0  1.0      0.0  0.0  1.0  12.0  2.0  4.0\n",
       " 1.0  1.0  1.0  1.0  27.0  1.0  0.0  0.0     20.0  1.0  0.0   8.0  4.0  7.0\n",
       " 1.0  1.0  1.0  1.0  34.0  1.0  1.0  0.0      7.0  1.0  0.0   9.0  5.0  4.0\n",
       " 1.0  1.0  1.0  1.0  24.0  1.0  0.0  0.0      0.0  0.0  0.0  12.0  3.0  3.0\n",
       " 1.0  1.0  0.0  1.0  31.0  0.0  0.0  0.0  …   5.0  0.0  0.0  13.0  4.0  4.0\n",
       " 1.0  1.0  1.0  1.0  33.0  1.0  0.0  0.0     30.0  1.0  0.0  11.0  4.0  2.0\n",
       " 1.0  1.0  1.0  1.0  27.0  1.0  0.0  0.0     30.0  1.0  0.0  10.0  4.0  5.0\n",
       " ⋮                         ⋮              ⋱                        ⋮    \n",
       " 1.0  1.0  1.0  1.0  30.0  0.0  0.0  0.0     30.0  1.0  0.0  11.0  2.0  2.0\n",
       " 1.0  0.0  1.0  1.0  19.0  0.0  0.0  0.0  …   2.0  0.0  0.0   7.0  6.0  6.0\n",
       " 1.0  1.0  0.0  1.0  37.0  0.0  0.0  0.0     30.0  1.0  0.0   9.0  2.0  1.0\n",
       " 1.0  1.0  0.0  1.0  28.0  0.0  0.0  0.0      0.0  0.0  0.0  10.0  4.0  3.0\n",
       " 1.0  1.0  1.0  1.0  27.0  0.0  0.0  1.0      5.0  0.0  1.0   9.0  4.0  5.0\n",
       " 1.0  1.0  0.0  1.0  38.0  0.0  0.0  0.0      0.0  0.0  0.0   7.0  6.0  2.0\n",
       " 1.0  0.0  1.0  1.0  27.0  0.0  0.0  0.0  …  30.0  0.0  1.0  11.0  2.0  3.0\n",
       " 1.0  0.0  1.0  1.0  37.0  0.0  0.0  0.0      0.0  0.0  0.0   6.0  4.0  1.0\n",
       " 1.0  0.0  1.0  1.0  29.0  1.0  0.0  1.0      0.0  1.0  1.0  10.0  3.0  6.0\n",
       " 1.0  1.0  1.0  1.0  25.0  0.0  0.0  1.0      0.0  1.0  0.0  13.0  6.0  4.0\n",
       " 1.0  1.0  1.0  1.0  18.0  0.0  0.0  0.0      0.0  1.0  0.0  11.0  2.0  4.0\n",
       " 1.0  1.0  1.0  1.0  25.0  0.0  0.0  1.0  …   0.0  0.0  0.0   9.0  6.0  2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transfer DataFrame to matrix form\n",
    "\n",
    "df=a_copy|>Tables.matrix\n",
    "\n",
    "# Transfer the dataset to 2-classifiers. df_0 represents the result is 0, df_1 represents the result is 1.\n",
    "# Due to my datasset is binary problems, and each result is 50 percents of the whole dataset. So i didn't add any other pre-actions for dataset. \n",
    "\n",
    "df_0 = df[df[:,1] .== 0, :]\n",
    "df_1 = df[df[:,1] .== 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weekly-presentation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21156×22 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  25.0  1.0  0.0  0.0  …   0.0  0.0  0.0  11.0  6.0  8.0\n",
       " 0.0  0.0  0.0  0.0  24.0  0.0  0.0  0.0      0.0  0.0  1.0   7.0  5.0  8.0\n",
       " 0.0  1.0  1.0  1.0  36.0  0.0  0.0  0.0      2.0  0.0  1.0  11.0  6.0  8.0\n",
       " 0.0  1.0  1.0  1.0  27.0  0.0  0.0  0.0      0.0  0.0  1.0   8.0  5.0  4.0\n",
       " 0.0  1.0  1.0  1.0  30.0  0.0  0.0  0.0     10.0  0.0  0.0   7.0  6.0  8.0\n",
       " 0.0  1.0  1.0  1.0  31.0  1.0  0.0  0.0  …   0.0  0.0  1.0   8.0  4.0  8.0\n",
       " 0.0  1.0  1.0  1.0  26.0  1.0  0.0  1.0     15.0  1.0  1.0   9.0  3.0  3.0\n",
       " 0.0  0.0  0.0  1.0  31.0  0.0  0.0  0.0      2.0  0.0  0.0   3.0  6.0  8.0\n",
       " 0.0  1.0  1.0  1.0  33.0  1.0  0.0  0.0      0.0  0.0  0.0   8.0  4.0  3.0\n",
       " 0.0  1.0  1.0  1.0  29.0  0.0  0.0  0.0      0.0  0.0  0.0  12.0  3.0  2.0\n",
       " 0.0  1.0  1.0  1.0  27.0  0.0  0.0  0.0  …   0.0  0.0  0.0  11.0  4.0  3.0\n",
       " 0.0  0.0  0.0  1.0  27.0  0.0  0.0  0.0      1.0  0.0  0.0   6.0  6.0  7.0\n",
       " 0.0  0.0  0.0  1.0  26.0  1.0  0.0  0.0      0.0  0.0  0.0   5.0  6.0  8.0\n",
       " ⋮                         ⋮              ⋱                        ⋮    \n",
       " 1.0  1.0  1.0  1.0  31.0  1.0  0.0  0.0      0.0  0.0  1.0  12.0  6.0  7.0\n",
       " 1.0  0.0  1.0  1.0  29.0  0.0  0.0  0.0  …  16.0  1.0  1.0   7.0  5.0  4.0\n",
       " 1.0  1.0  0.0  1.0  30.0  0.0  0.0  0.0      0.0  0.0  1.0   9.0  4.0  7.0\n",
       " 1.0  1.0  1.0  1.0  26.0  0.0  0.0  0.0     30.0  0.0  0.0  11.0  2.0  2.0\n",
       " 1.0  1.0  0.0  1.0  30.0  0.0  0.0  0.0      0.0  0.0  0.0  11.0  4.0  3.0\n",
       " 1.0  0.0  1.0  1.0  35.0  0.0  0.0  0.0      6.0  1.0  0.0  10.0  6.0  5.0\n",
       " 1.0  1.0  1.0  1.0  29.0  1.0  0.0  0.0  …   0.0  1.0  0.0  13.0  2.0  1.0\n",
       " 1.0  1.0  0.0  1.0  32.0  0.0  1.0  0.0      0.0  0.0  1.0   9.0  6.0  8.0\n",
       " 1.0  1.0  0.0  1.0  30.0  0.0  0.0  1.0      4.0  0.0  0.0  12.0  5.0  2.0\n",
       " 1.0  1.0  1.0  1.0  39.0  1.0  0.0  1.0     30.0  1.0  1.0  10.0  4.0  1.0\n",
       " 1.0  0.0  0.0  1.0  25.0  0.0  0.0  0.0      1.0  0.0  0.0   9.0  5.0  2.0\n",
       " 1.0  1.0  1.0  1.0  30.0  0.0  0.0  1.0  …  30.0  1.0  1.0  10.0  2.0  2.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The columns of matrix is 35346(here just using 35000 is the same), using random sub-sequence to select 70 percents of \n",
    "# columns as the train data.\n",
    "\n",
    "sample = randsubseq(1:35000, 0.7)\n",
    "train_df = vcat(df_0[sample, :], df_1[sample, :])\n",
    "\n",
    "# Then from the not selected columns (which is 30 percents) to select the test data.\n",
    "\n",
    "notsample = [i for i in 1:35000 if isempty(searchsorted(sample, i))]\n",
    "test_df = vcat(df_0[notsample, :], df_1[notsample, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "featured-residence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21156-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " ⋮\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide the columns into features and result. From my dataset, From 2 to 22 columns are the attributes of whether diabetes or not.\n",
    "\n",
    "X_train = train_df[:, 2:22]\n",
    "X_test = test_df[:, 2:22]\n",
    "\n",
    "y_train = train_df[:, 1]\n",
    "y_test = test_df[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prerequisite-commercial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-1.1434716619741414 0.8745118571018049 … 0.8745118571018049 0.8745118571018049; 0.9474032514701192 0.9474032514701192 … -1.0554951390599407 -1.0554951390599407; … ; 0.07697766693135667 0.07697766693135667 … -1.8656196792816078 -0.8943210061751256; 0.5955156758064295 1.0563786666816453 … -0.7870732968192181 -0.7870732968192181], [-1.1434716619741414 -1.1434716619741414 … -1.1434716619741414 0.8745118571018049; -1.0554951390599407 -1.0554951390599407 … -1.0554951390599407 0.9474032514701192; … ; 1.048276340037839 0.07697766693135667 … 0.07697766693135667 -2.83691835238809; 1.0563786666816453 1.0563786666816453 … -1.70879927856965 -1.70879927856965])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributions\n",
    "\n",
    "# Transfer the X_train to 1 diminension.\n",
    "\n",
    "dt = fit(ZScoreTransform, X_train, dims=1)\n",
    "\n",
    "# Using StateBase package to transfer X_train and X_test(make them standard) to the same formate as dt.\n",
    "\n",
    "X_train_std = StatsBase.transform(dt, X_train)\n",
    "X_test_std = StatsBase.transform(dt, X_test)\n",
    "\n",
    "# Transpose the X_train_std and X_test_std.\n",
    "    \n",
    "X_train_std, X_test_std= transpose(X_train_std), transpose(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-neighbor",
   "metadata": {},
   "source": [
    "Construct the neural network\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "african-recall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialize_model_weights"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Funtion to initialise the parameters or weights of the desired network.\n",
    "    In my example, layer_dims=3.\n",
    "\"\"\"\n",
    "function initialize_model_weights(layer_dims, seed = 7)\n",
    "    params = Dict()\n",
    "\n",
    "    for l=2:length(layer_dims)\n",
    "        params[string(\"W_\", (l-1))] = rand(MersenneTwister(seed), Uniform(-1, 1), layer_dims[l], layer_dims[l-1]) * sqrt(6 / (layer_dims[l]+layer_dims[l-1]))\n",
    "        params[string(\"b_\", (l-1))] = zeros(layer_dims[l], 1)\n",
    "    end\n",
    "    return params\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "essential-garden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_cost (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   sigmiod function for activation\n",
    "\n",
    "function sd_sigmoid(Z)\n",
    "    A = 1 ./ (1 .+ exp.(.-Z))\n",
    "    return (A = A, Z = Z)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    Make a forward activation from a linear forward.\n",
    "    For example:\n",
    "        Z_prev=W_1*A_0+b_1\n",
    "        Using sigmoid function activates\n",
    "        DO it second time in second layer(which is last layer in my example)\n",
    "        Z_prev=W_2*A_!+b_2\n",
    "\"\"\"\n",
    "\n",
    "function forward_activation(A_prev, W, b, activation_function=\"sigmoid\")\n",
    "    Z = (W * A_prev) .+ b\n",
    "    linear_cache = (A_prev, W, b)\n",
    "    A, activation_cache = sd_sigmoid(Z)\n",
    "\n",
    "    cache = (linear_step_cache=linear_cache, activation_step_cache=activation_cache)\n",
    "    @assert size(A) == (size(W, 1), size(A_prev, 2))\n",
    "\n",
    "    return A, cache\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Forward the design matrix through the network layers using the parameters.\n",
    "\"\"\"\n",
    "\n",
    "function forward_propagate(input, parameters)\n",
    "    master_cache = []\n",
    "    A = input\n",
    "    L = Int(length(parameters) / 2)\n",
    "\n",
    "    # Forward propagate until the last (output) layer :(the last layer is 2 here)\n",
    "\n",
    "    for l = 1 : (L-1)\n",
    "        A_prev = A\n",
    "        A, cache = forward_activation(A_prev, parameters[string(\"W_\", (l))], parameters[string(\"b_\", (l))], \"sigmoid\")\n",
    "        push!(master_cache , cache)\n",
    "    end\n",
    "\n",
    "    # Make predictions store in Ŷ\n",
    "    \n",
    "    Ŷ, cache = forward_activation(A, parameters[string(\"W_\", (L))], parameters[string(\"b_\", (L))], \"sigmoid\")\n",
    "    push!(master_cache, cache)\n",
    "\n",
    "    return Ŷ, master_cache\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    Computes the log loss (binary cross entropy) of the current predictions.\n",
    "\"\"\"\n",
    "\n",
    "function calculate_cost(Ŷ, Y)\n",
    "    m = max(size(Y, 2), size(Y, 1))\n",
    "    epsilon = eps(1.0)\n",
    "\n",
    "    Ŷ_new = [max(i, epsilon) for i in Ŷ]\n",
    "    Ŷ_new = [min(i, 1-epsilon) for i in Ŷ_new]\n",
    "\n",
    "    cost = -sum(Y .* log.(Ŷ_new) + (1 .- Y) .* log.(1 .- Ŷ_new)) / m\n",
    "    return cost\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "appointed-treatment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activation_backward (generic function with 2 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   sigmoid function for backwards\n",
    "\n",
    "function sigmoid_backwards(∂A, activated_cache)\n",
    "    s = sd_sigmoid(activated_cache).A\n",
    "    ∂Z = ∂A .* s .* (1 .- s)\n",
    "\n",
    "    @assert (size(∂Z) == size(activated_cache))\n",
    "    return ∂Z\n",
    "end\n",
    "\n",
    "\n",
    "function activation_backward(∂A, cache, activation_function=\"sigmoid\")\n",
    "    linear_cache , cache_activation = cache\n",
    "    ∂Z = sigmoid_backwards(∂A , cache_activation)\n",
    "    \n",
    "    A_prev , W , b = linear_cache\n",
    "    m = max(size(A_prev, 2), size(A_prev, 1))\n",
    "    \n",
    "    ∂W = ∂Z * (A_prev') / m\n",
    "    ∂b = sum(∂Z, dims = 2) / m\n",
    "    ∂A_prev = (W') * ∂Z\n",
    "\n",
    "    return ∂W , ∂b , ∂A_prev\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "civic-vaccine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "back_propagate (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function back_propagate(Ŷ, Y, master_cache)\n",
    "    ∇ = Dict()\n",
    "\n",
    "    L = length(master_cache)\n",
    "    Y = reshape(Y , size(Ŷ))\n",
    "\n",
    "    # Partial derivative of the output layer\n",
    "\n",
    "    ∂Ŷ = (-(Y ./ Ŷ) .+ ((1 .- Y) ./ ( 1 .- Ŷ)))\n",
    "    current_cache = master_cache[L]\n",
    "\n",
    "    # Backpropagate on the layer preceeding the output layer\n",
    "\n",
    "    ∇[string(\"∂W_\", (L))], ∇[string(\"∂b_\", (L))], ∇[string(\"∂A_\", (L-1))] = activation_backward(∂Ŷ, current_cache, \"sigmoid\")\n",
    "    \n",
    "    # Go backwards in the layers and compute the partial derivates of each component.\n",
    "    for l=reverse(0:L-2)\n",
    "        current_cache = master_cache[l+1]\n",
    "        ∇[string(\"∂W_\", (l+1))], ∇[string(\"∂b_\", (l+1))], ∇[string(\"∂A_\", (l))] = activation_backward(∇[string(\"∂A_\", (l+1))],\n",
    "                                                                                                             current_cache,\n",
    "                                                                                                             \"sigmoid\")\n",
    "    end\n",
    "    return ∇\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "smart-commons",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Gradiant desecents ways to update model weights\n",
    "\"\"\"\n",
    "function update_model_weights(parameters, ∇, η)\n",
    "    L = Int(length(parameters) / 2)\n",
    "\n",
    "    for l = 0: (L-1)\n",
    "        parameters[string(\"W_\", (l + 1))] -= η .* ∇[string(\"∂W_\", (l + 1))]\n",
    "        parameters[string(\"b_\", (l + 1))] -= η .* ∇[string(\"∂b_\", (l + 1))]\n",
    "    end\n",
    "\n",
    "    return parameters\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    if the predicted result is larger than 0.5 return 1 and compared with the true result.\n",
    "    Than's how we calculate accuracy of the whole dataset\n",
    "\"\"\"\n",
    "\n",
    "function calculate_accuracy(Ŷ , Y)\n",
    "    @assert size(Ŷ) == size(Y)\n",
    "    return sum((Ŷ .> 0.5) .== Y) / length(Y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "supreme-marker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false_false_counts (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    In order to compute for confusion matrix\n",
    "\"\"\"\n",
    "\n",
    "function true_true_counts(Ŷ , Y)\n",
    "    @assert size(Ŷ) == size(Y)\n",
    "    return sum(Y[(Ŷ .> 0.5)] .== 1)\n",
    "end\n",
    "function true_false_counts(Ŷ , Y)\n",
    "    @assert size(Ŷ) == size(Y)\n",
    "    return sum(Y[(Ŷ .> 0.5)] .== 0)\n",
    "end\n",
    "function false_true_counts(Ŷ , Y)\n",
    "    @assert size(Ŷ) == size(Y)\n",
    "    return sum(Y[(Ŷ .< 0.5)] .== 1)\n",
    "end\n",
    "function false_false_counts(Ŷ , Y)\n",
    "    @assert size(Ŷ) == size(Y)\n",
    "    return sum(Y[(Ŷ .< 0.5)] .== 0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "through-awareness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_model_weights_counts (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    In order to count how many weights are updated during the gradiants descents\n",
    "\"\"\"\n",
    "\n",
    "function update_model_weights_counts(parameters_old,parameters_new)\n",
    "    L=Int(length(parameters_old) / 2)\n",
    "    count=0\n",
    "    for l = 0 : (L-1)\n",
    "        for m=1:Int(size(parameters_old[string(\"W_\",(l+1))])[L])\n",
    "            if parameters_old[string(\"W_\",(l+1))]!=parameters_new[string(\"W_\",(l+1))]\n",
    "                count=count+1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return count\n",
    "end\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "together-continuity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_network (generic function with 2 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_network(layer_dims, X_train, Y_train, X_test, Y_test, init_params=nothing; η=0.01, epochs=0.1, seed=8, verbose=true)\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "    updates=[]\n",
    "\n",
    "    if init_params == nothing\n",
    "        params = initialize_model_weights(layer_dims, seed)\n",
    "    else\n",
    "        params = init_params\n",
    "    end\n",
    "    initial_params = copy(params)\n",
    "\n",
    "    for i = 1:epochs\n",
    "        Ŷ_train, caches  = forward_propagate(X_train, params)\n",
    "        Ŷ_test = forward_propagate(X_test, params)[1]\n",
    "        cost_train = calculate_cost(Ŷ_train, Y_train)\n",
    "        cost_test = calculate_cost(Ŷ_test, Y_test)\n",
    "        acc_train = calculate_accuracy(Ŷ_train, Y_train)\n",
    "        acc_test = calculate_accuracy(Ŷ_test, Y_test)\n",
    "        ∇  = back_propagate(Ŷ_train, Y_train, caches)\n",
    "        params = update_model_weights(params, ∇, η)\n",
    "        update_counts = update_model_weights_counts(initial_params,params)\n",
    "        \n",
    "        true_true_result = true_true_counts(Ŷ_test, Y_test)\n",
    "        true_false_result = true_false_counts(Ŷ_test, Y_test)\n",
    "        false_true_result = false_true_counts(Ŷ_test, Y_test)\n",
    "        false_false_result = false_false_counts(Ŷ_test, Y_test)\n",
    "        \n",
    "        if verbose && i%10 == 1\n",
    "            \n",
    "            # keep track of performance on test data\n",
    "            \n",
    "            println(\"Iteration -> $i, test accuracy -> $acc_test\")\n",
    "            println(\"confusion matrix of test data:\")\n",
    "            println(\"true_true_result->$true_true_result,  true_false_result->$true_false_result\")\n",
    "            println(\"false_false_result->$false_false_result,   false_true_result->$false_true_result\")\n",
    "        end\n",
    "        \n",
    "\n",
    "        push!(accuracy_train , acc_train)\n",
    "        push!(accuracy_test , acc_test)\n",
    "        push!(updates , update_counts)\n",
    "\n",
    "    end\n",
    "    \n",
    "    return (initial_params = initial_params, accuracy_train = accuracy_train, accuracy_test = accuracy_test, params = params, updates = updates)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-script",
   "metadata": {},
   "source": [
    "Repeat it ten times with different initial weights\n",
    "======\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "extended-clearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration -> 1, test accuracy -> 0.5002836074872377\n",
      "confusion matrix of test data:\n",
      "true_true_result->10574,  true_false_result->10568\n",
      "false_false_result->10,   false_true_result->4\n",
      "Iteration -> 11, test accuracy -> 0.5001418037436188\n",
      "confusion matrix of test data:\n",
      "true_true_result->10570,  true_false_result->10567\n",
      "false_false_result->11,   false_true_result->8\n",
      "Iteration -> 21, test accuracy -> 0.5000945358290793\n",
      "confusion matrix of test data:\n",
      "true_true_result->10568,  true_false_result->10566\n",
      "false_false_result->12,   false_true_result->10\n",
      "Iteration -> 31, test accuracy -> 0.5000945358290793\n",
      "confusion matrix of test data:\n",
      "true_true_result->10566,  true_false_result->10564\n",
      "false_false_result->14,   false_true_result->12\n",
      "Iteration -> 41, test accuracy -> 0.5003308754017772\n",
      "confusion matrix of test data:\n",
      "true_true_result->10566,  true_false_result->10559\n",
      "false_false_result->19,   false_true_result->12\n",
      "Iteration -> 51, test accuracy -> 0.5003308754017772\n",
      "confusion matrix of test data:\n",
      "true_true_result->10566,  true_false_result->10559\n",
      "false_false_result->19,   false_true_result->12\n",
      "Iteration -> 61, test accuracy -> 0.5004254112308565\n",
      "confusion matrix of test data:\n",
      "true_true_result->10564,  true_false_result->10555\n",
      "false_false_result->23,   false_true_result->14\n",
      "Iteration -> 71, test accuracy -> 0.5006617508035546\n",
      "confusion matrix of test data:\n",
      "true_true_result->10564,  true_false_result->10550\n",
      "false_false_result->28,   false_true_result->14\n",
      "Iteration -> 81, test accuracy -> 0.5010871620344111\n",
      "confusion matrix of test data:\n",
      "true_true_result->10563,  true_false_result->10540\n",
      "false_false_result->38,   false_true_result->15\n",
      "Iteration -> 91, test accuracy -> 0.5013707695216487\n",
      "confusion matrix of test data:\n",
      "true_true_result->10561,  true_false_result->10532\n",
      "false_false_result->46,   false_true_result->17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(initial_params = Dict{Any, Any}(\"b_1\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"W_2\" => [-0.25222848450245616 0.4901194841274318 … 0.36165252253738 0.48085781603264494], \"b_2\" => [0.0;;], \"W_1\" => [-0.18254957478507733 -0.15199929756930958 … -0.06409891974181271 0.1451668594976021; 0.35472243984589646 0.22051460054561445 … -0.3509024580686565 0.006683520845412009; … ; 0.26174487920893974 -0.21459964946448598 … 0.10644930538985586 0.09045826224266647; 0.34801933660266465 -0.258073800550735 … -0.0813921532404188 -0.19645632760665652]), accuracy_train = Any[0.5003275734993039, 0.5003071001555974, 0.5003275734993039, 0.5003275734993039, 0.5003275734993039, 0.5003275734993039, 0.5003480468430104, 0.5003480468430104, 0.5003889935304234, 0.5003889935304234  …  0.501617394152813, 0.501658340840226, 0.501699287527639, 0.501740234215052, 0.501781180902465, 0.501781180902465, 0.501822127589878, 0.501822127589878, 0.5018630742772909, 0.501904020964704], accuracy_test = Any[0.5002836074872377, 0.5002836074872377, 0.5002363395726981, 0.5001890716581584, 0.5001890716581584, 0.5002363395726981, 0.5002363395726981, 0.5001890716581584, 0.5001890716581584, 0.5001418037436188  …  0.5013707695216487, 0.5013707695216487, 0.5013707695216487, 0.5012762336925695, 0.5012762336925695, 0.5012762336925695, 0.5012762336925695, 0.5012289657780299, 0.5013235016071091, 0.5013707695216487], params = Dict{Any, Any}(\"b_1\" => [0.0015054568218514626; -0.0023557532312211307; … ; -0.0020274906265778108; -0.0026255940357729174;;], \"W_2\" => [-0.2673234341500931 0.48091026056983277 … 0.34622395925486876 0.4700182964113213], \"b_2\" => [-0.026009472738300204;;], \"W_1\" => [-0.18323447278103777 -0.15267945978869557 … -0.06361266081311562 0.14581557230524017; 0.3556399838539197 0.22143900968948563 … -0.3516181036458355 0.005635669484656839; … ; 0.26275199304315966 -0.21371399938487612 … 0.10581730298099537 0.08959126995362694; 0.34921060226762013 -0.2567862417058666 … -0.08224623406424868 -0.19759992701803458]), updates = Any[42, 42, 42, 42, 42, 42, 42, 42, 42, 42  …  42, 42, 42, 42, 42, 42, 42, 42, 42, 42])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params_1,accuracy_train_1,accuracy_test_1,params_1,update_1=train_network([21, 21, 1], X_train_std, transpose(y_train), X_test_std, transpose(y_test); η=0.001, epochs=100, seed=12, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "monetary-veteran",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration -> 1, test accuracy -> 0.5078464738135754\n",
      "confusion matrix of test data:\n",
      "true_true_result->10391,  true_false_result->10225\n",
      "false_false_result->353,   false_true_result->187\n",
      "Iteration -> 11, test accuracy -> 0.5091227075061449\n",
      "confusion matrix of test data:\n",
      "true_true_result->10375,  true_false_result->10182\n",
      "false_false_result->396,   false_true_result->203\n",
      "Iteration -> 21, test accuracy -> 0.5100680657969371\n",
      "confusion matrix of test data:\n",
      "true_true_result->10354,  true_false_result->10141\n",
      "false_false_result->437,   false_true_result->224\n",
      "Iteration -> 31, test accuracy -> 0.5112497636604273\n",
      "confusion matrix of test data:\n",
      "true_true_result->10329,  true_false_result->10091\n",
      "false_false_result->487,   false_true_result->249\n",
      "Iteration -> 41, test accuracy -> 0.5127150690111553\n",
      "confusion matrix of test data:\n",
      "true_true_result->10313,  true_false_result->10044\n",
      "false_false_result->534,   false_true_result->265\n",
      "Iteration -> 51, test accuracy -> 0.5144639818491208\n",
      "confusion matrix of test data:\n",
      "true_true_result->10295,  true_false_result->9989\n",
      "false_false_result->589,   false_true_result->283\n",
      "Iteration -> 61, test accuracy -> 0.5153620722253734\n",
      "confusion matrix of test data:\n",
      "true_true_result->10262,  true_false_result->9937\n",
      "false_false_result->641,   false_true_result->316\n",
      "Iteration -> 71, test accuracy -> 0.5166383059179429\n",
      "confusion matrix of test data:\n",
      "true_true_result->10236,  true_false_result->9884\n",
      "false_false_result->694,   false_true_result->342\n",
      "Iteration -> 81, test accuracy -> 0.5187180941576858\n",
      "confusion matrix of test data:\n",
      "true_true_result->10215,  true_false_result->9819\n",
      "false_false_result->759,   false_true_result->363\n",
      "Iteration -> 91, test accuracy -> 0.5203724711665721\n",
      "confusion matrix of test data:\n",
      "true_true_result->10185,  true_false_result->9754\n",
      "false_false_result->824,   false_true_result->393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(initial_params = Dict{Any, Any}(\"b_1\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"W_2\" => [0.4571632612623438 0.48281600816652065 … 0.2797522584996748 -0.044651100354088785], \"b_2\" => [0.0;;], \"W_1\" => [0.33087047688298427 0.2991765939840969 … -0.21887642878472888 -0.293459665012528; 0.3494365720195591 0.28355764076298823 … -0.10252894066702041 0.24063053268274867; … ; 0.2024698199135529 0.1611890640865404 … -0.10327966086948012 -0.07009459293543113; -0.0323160938757706 -0.23744261188826996 … 0.2559144629882251 0.35434565262852136]), accuracy_train = Any[0.5076160838588158, 0.5077798706084677, 0.5078003439521742, 0.5078822373270002, 0.5079641307018262, 0.5080255507329458, 0.5080460240766522, 0.5080664974203587, 0.5082917042011301, 0.5082917042011301  …  0.5183850626484318, 0.5184874293669642, 0.5185693227417902, 0.5186512161166161, 0.5189583162722136, 0.5191221030218656, 0.519347309802637, 0.5195725165834084, 0.5196953566456474, 0.5200229301449513], accuracy_test = Any[0.5078464738135754, 0.5079410096426545, 0.508130081300813, 0.5081773492153526, 0.5082718850444319, 0.5086027604462091, 0.5086972962752884, 0.5088391000189072, 0.5090281716770656, 0.5090281716770656  …  0.5203724711665721, 0.5204670069956514, 0.5206088107392702, 0.5207506144828891, 0.5207978823974286, 0.5208924182265079, 0.5214123652864435, 0.5216487048591416, 0.5218377765173, 0.5221686519190774], params = Dict{Any, Any}(\"b_1\" => [-0.0015518808659474967; -0.001629403988418186; … ; -0.000979349501158953; 0.0001633964210256684;;], \"W_2\" => [0.44988803904747643 0.4765358583096961 … 0.27174480163790754 -0.0556948137689687], \"b_2\" => [-0.016441043378812463;;], \"W_1\" => [0.3319106015676582 0.3001246365620996 … -0.2194540897050569 -0.29443104185384184; 0.3504666891654233 0.284503842028621 … -0.10313698664352085 0.23955912478025498; … ; 0.20314947907999403 0.16180789250477914 … -0.10366846734958433 -0.07073142469886597; -0.03241784815215284 -0.23753255544281487 … 0.25596139245622895 0.35443667649375366]), updates = Any[42, 42, 42, 42, 42, 42, 42, 42, 42, 42  …  42, 42, 42, 42, 42, 42, 42, 42, 42, 42])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params_2,accuracy_train_2,accuracy_test_2,params_2,update_2=train_network([21, 21, 1], X_train_std, transpose(y_train), X_test_std, transpose(y_test); η=0.001, epochs=100, seed=13, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "lasting-western",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration -> 1, test accuracy -> 0.507515598411798\n",
      "confusion matrix of test data:\n",
      "true_true_result->568,  true_false_result->409\n",
      "false_false_result->10169,   false_true_result->10010\n",
      "Iteration -> 11, test accuracy -> 0.508366420873511\n",
      "confusion matrix of test data:\n",
      "true_true_result->618,  true_false_result->441\n",
      "false_false_result->10137,   false_true_result->9960\n",
      "Iteration -> 21, test accuracy -> 0.5090754395916052\n",
      "confusion matrix of test data:\n",
      "true_true_result->664,  true_false_result->472\n",
      "false_false_result->10106,   false_true_result->9914\n",
      "Iteration -> 31, test accuracy -> 0.5095481187370013\n",
      "confusion matrix of test data:\n",
      "true_true_result->710,  true_false_result->508\n",
      "false_false_result->10070,   false_true_result->9868\n",
      "Iteration -> 41, test accuracy -> 0.5105880128568727\n",
      "confusion matrix of test data:\n",
      "true_true_result->760,  true_false_result->536\n",
      "false_false_result->10042,   false_true_result->9818\n",
      "Iteration -> 51, test accuracy -> 0.5121951219512195\n",
      "confusion matrix of test data:\n",
      "true_true_result->820,  true_false_result->562\n",
      "false_false_result->10016,   false_true_result->9758\n",
      "Iteration -> 61, test accuracy -> 0.5130932123274721\n",
      "confusion matrix of test data:\n",
      "true_true_result->878,  true_false_result->601\n",
      "false_false_result->9977,   false_true_result->9700\n",
      "Iteration -> 71, test accuracy -> 0.5141331064473436\n",
      "confusion matrix of test data:\n",
      "true_true_result->939,  true_false_result->640\n",
      "false_false_result->9938,   false_true_result->9639\n",
      "Iteration -> 81, test accuracy -> 0.5145585176782\n",
      "confusion matrix of test data:\n",
      "true_true_result->992,  true_false_result->684\n",
      "false_false_result->9894,   false_true_result->9586\n",
      "Iteration -> 91, test accuracy -> 0.5151257326526754\n",
      "confusion matrix of test data:\n",
      "true_true_result->1033,  true_false_result->713\n",
      "false_false_result->9865,   false_true_result->9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(initial_params = Dict{Any, Any}(\"b_1\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"W_2\" => [-0.13468563322998187 0.5200010938934482 … -0.30220196381679754 0.5121834580738713], \"b_2\" => [0.0;;], \"W_1\" => [-0.09747830473743599 0.18564089534919148 … 0.22401825063756373 0.1702435249278228; 0.376349161218941 0.13209363936312066 … 0.2508391513073533 0.12698911803366686; … ; -0.21871772374477597 0.07134757245374349 … 0.1412765488383628 0.23757320291915493; 0.37069117180706534 -0.11761735359165301 … 0.2889347636556209 0.18270117780820405]), accuracy_train = Any[0.5073499303906314, 0.5074522971091638, 0.5074932437965769, 0.5075956105151094, 0.5076160838588158, 0.5076570305462288, 0.5076979772336419, 0.5077798706084677, 0.5078412906395873, 0.5078822373270002  …  0.5132257800343952, 0.5134509868151667, 0.5135943002211121, 0.5136761935959381, 0.5135943002211121, 0.5136966669396446, 0.5138195070018835, 0.5138399803455901, 0.514003767095242, 0.5141266071574809], accuracy_test = Any[0.507515598411798, 0.507515598411798, 0.5077046700699566, 0.5077519379844961, 0.5077046700699566, 0.5077046700699566, 0.5077992058990357, 0.5078464738135754, 0.5081773492153526, 0.5082246171298922  …  0.5151257326526754, 0.5151730005672149, 0.5152675363962942, 0.5155511438835318, 0.5157402155416903, 0.5156456797126111, 0.5158347513707695, 0.5159292871998488, 0.5160710909434676, 0.5161183588580072], params = Dict{Any, Any}(\"b_1\" => [-0.0003179795771733263; 0.0012834231121069634; … ; -0.0007031643881550157; 0.0011599242748109922;;], \"W_2\" => [-0.1257222949579503 0.5247774434384234 … -0.295714896347397 0.5165697881060892], \"b_2\" => [0.01187494473546569;;], \"W_1\" => [-0.09781284560874418 0.1852904677748773 … 0.2242879309237841 0.17053111029398246; 0.3778789318632656 0.13367711887860698 … 0.24989530541944174 0.12591330663053948; … ; -0.21953299434537069 0.0704768288312158 … 0.1418507546287829 0.2382109331506453; 0.3720298002983161 -0.11623717465975833 … 0.2881099740136118 0.1817832352108553]), updates = Any[42, 42, 42, 42, 42, 42, 42, 42, 42, 42  …  42, 42, 42, 42, 42, 42, 42, 42, 42, 42])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params_3,accuracy_train_3,accuracy_test_3,params_3,update_3=train_network([21, 21, 1], X_train_std, transpose(y_train), X_test_std, transpose(y_test); η=0.001, epochs=100, seed=14, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "worthy-clerk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration -> 1, test accuracy -> 0.6448288901493666\n",
      "confusion matrix of test data:\n",
      "true_true_result->7413,  true_false_result->4349\n",
      "false_false_result->6229,   false_true_result->3165\n",
      "Iteration -> 11, test accuracy -> 0.645017961807525\n",
      "confusion matrix of test data:\n",
      "true_true_result->7411,  true_false_result->4343\n",
      "false_false_result->6235,   false_true_result->3167\n",
      "Iteration -> 21, test accuracy -> 0.6453961051238419\n",
      "confusion matrix of test data:\n",
      "true_true_result->7407,  true_false_result->4331\n",
      "false_false_result->6247,   false_true_result->3171\n",
      "Iteration -> 31, test accuracy -> 0.6458215163546984\n",
      "confusion matrix of test data:\n",
      "true_true_result->7407,  true_false_result->4322\n",
      "false_false_result->6256,   false_true_result->3171\n",
      "Iteration -> 41, test accuracy -> 0.6466723388164114\n",
      "confusion matrix of test data:\n",
      "true_true_result->7412,  true_false_result->4309\n",
      "false_false_result->6269,   false_true_result->3166\n",
      "Iteration -> 51, test accuracy -> 0.6471450179618076\n",
      "confusion matrix of test data:\n",
      "true_true_result->7412,  true_false_result->4299\n",
      "false_false_result->6279,   false_true_result->3166\n",
      "Iteration -> 61, test accuracy -> 0.6474286254490452\n",
      "confusion matrix of test data:\n",
      "true_true_result->7414,  true_false_result->4295\n",
      "false_false_result->6283,   false_true_result->3164\n",
      "Iteration -> 71, test accuracy -> 0.6474758933635848\n",
      "confusion matrix of test data:\n",
      "true_true_result->7412,  true_false_result->4292\n",
      "false_false_result->6286,   false_true_result->3166\n",
      "Iteration -> 81, test accuracy -> 0.6479485725089809\n",
      "confusion matrix of test data:\n",
      "true_true_result->7413,  true_false_result->4283\n",
      "false_false_result->6295,   false_true_result->3165\n",
      "Iteration -> 91, test accuracy -> 0.6480903762525997\n",
      "confusion matrix of test data:\n",
      "true_true_result->7411,  true_false_result->4278\n",
      "false_false_result->6300,   false_true_result->3167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(initial_params = Dict{Any, Any}(\"b_1\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"W_2\" => [-0.04336310900296086 -0.021872424284187016 … 0.233858496055723 -0.23293950408708602], \"b_2\" => [0.0;;], \"W_1\" => [-0.03138391417394565 -0.30887129525187956 … -0.04952106219768717 0.27701775350946856; -0.015830098493726064 0.3758204231158233 … -0.24388455854678187 0.18825412019664212; … ; 0.16925435324666593 -0.3309789497739253 … 0.07325751570332313 0.268083908905785; -0.1685892356909049 -0.026371965207151016 … 0.07643802363126354 0.1272007694973479]), accuracy_train = Any[0.6515232167717632, 0.6515436901154696, 0.6516051101465892, 0.6516255834902956, 0.6516255834902956, 0.6516460568340021, 0.6517074768651216, 0.6516870035214151, 0.6516870035214151, 0.6516870035214151  …  0.6538162312668905, 0.6538571779543035, 0.6538981246417165, 0.65387765129801, 0.6538981246417165, 0.6539390713291294, 0.653959544672836, 0.6539800180165425, 0.6540414380476619, 0.6540209647039554], accuracy_test = Any[0.6448288901493666, 0.6447343543202874, 0.6448761580639062, 0.6449234259784459, 0.645017961807525, 0.645017961807525, 0.645017961807525, 0.6451124976366043, 0.6451124976366043, 0.6450652297220647  …  0.6480903762525997, 0.6482321799962185, 0.6482321799962185, 0.6481849120816789, 0.6482321799962185, 0.6482794479107582, 0.6482794479107582, 0.6483739837398373, 0.648421251654377, 0.648421251654377], params = Dict{Any, Any}(\"b_1\" => [8.603835568760855e-6; 4.5440598141507485e-6; … ; -3.173042196184841e-5; 5.685215843965506e-5;;], \"W_2\" => [-0.04535110059669956 -0.020098349931733806 … 0.23184972376557061 -0.2332214247633366], \"b_2\" => [-0.0006469435766053675;;], \"W_1\" => [-0.03150497939149155 -0.3089908654629292 … -0.04947053747286974 0.2770886576064761; -0.015885026802242862 0.37576669085942666 … -0.24386584662039157 0.18828614148743994; … ; 0.1698926550509147 -0.3303646594425579 … 0.07303310919282213 0.26774185763155744; -0.16924336751071925 -0.02702109461115497 … 0.0766998274197826 0.12759550636562103]), updates = Any[42, 42, 42, 42, 42, 42, 42, 42, 42, 42  …  42, 42, 42, 42, 42, 42, 42, 42, 42, 42])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params_4,accuracy_train_4,accuracy_test_4,params_4,update_4=train_network([21, 21, 1], X_train_std, transpose(y_train), X_test_std, transpose(y_test); η=0.001, epochs=100, seed=15, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "worldwide-relief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration -> 1, test accuracy -> 0.6757421062582719\n",
      "confusion matrix of test data:\n",
      "true_true_result->7842,  true_false_result->4124\n",
      "false_false_result->6454,   false_true_result->2736\n",
      "Iteration -> 11, test accuracy -> 0.6758366420873511\n",
      "confusion matrix of test data:\n",
      "true_true_result->7834,  true_false_result->4114\n",
      "false_false_result->6464,   false_true_result->2744\n",
      "Iteration -> 21, test accuracy -> 0.6760257137455096\n",
      "confusion matrix of test data:\n",
      "true_true_result->7830,  true_false_result->4106\n",
      "false_false_result->6472,   false_true_result->2748\n",
      "Iteration -> 31, test accuracy -> 0.6763565891472868\n",
      "confusion matrix of test data:\n",
      "true_true_result->7825,  true_false_result->4094\n",
      "false_false_result->6484,   false_true_result->2753\n",
      "Iteration -> 41, test accuracy -> 0.6762620533182075\n",
      "confusion matrix of test data:\n",
      "true_true_result->7817,  true_false_result->4088\n",
      "false_false_result->6490,   false_true_result->2761\n",
      "Iteration -> 51, test accuracy -> 0.6762620533182075\n",
      "confusion matrix of test data:\n",
      "true_true_result->7807,  true_false_result->4078\n",
      "false_false_result->6500,   false_true_result->2771\n",
      "Iteration -> 61, test accuracy -> 0.6765929287199849\n",
      "confusion matrix of test data:\n",
      "true_true_result->7806,  true_false_result->4070\n",
      "false_false_result->6508,   false_true_result->2772\n",
      "Iteration -> 71, test accuracy -> 0.6764511249763661\n",
      "confusion matrix of test data:\n",
      "true_true_result->7797,  true_false_result->4064\n",
      "false_false_result->6514,   false_true_result->2781\n",
      "Iteration -> 81, test accuracy -> 0.6764511249763661\n",
      "confusion matrix of test data:\n",
      "true_true_result->7792,  true_false_result->4059\n",
      "false_false_result->6519,   false_true_result->2786\n",
      "Iteration -> 91, test accuracy -> 0.6760729816600491\n",
      "confusion matrix of test data:\n",
      "true_true_result->7779,  true_false_result->4054\n",
      "false_false_result->6524,   false_true_result->2799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(initial_params = Dict{Any, Any}(\"b_1\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"W_2\" => [0.40847788493824344 0.41420926500254435 … 0.08365893547319866 0.009428730148629223], \"b_2\" => [0.0;;], \"W_1\" => [0.2956345884235686 -0.23265297532675355 … -0.085501618393267 -0.003645335538989069; 0.2997826567741106 -0.03650797790071291 … 0.32095099064749105 0.04766629183858145; … ; 0.060547892232433106 -0.11341390539202623 … 0.015043628889169488 0.11292192346493678; 0.006824013880869759 -0.1243747642465575 … -0.11351174409263617 0.00417808648545372]), accuracy_train = Any[0.678118090246499, 0.678118090246499, 0.6781795102776186, 0.6782614036524445, 0.6782818769961511, 0.6783842437146835, 0.6784251904020965, 0.6783842437146835, 0.6784251904020965, 0.678363770370977  …  0.6791622307755303, 0.6791622307755303, 0.6791417574318238, 0.6791008107444108, 0.6791417574318238, 0.6791212840881172, 0.6791212840881172, 0.6791622307755303, 0.6792031774629432, 0.6792236508066497], accuracy_test = Any[0.6757421062582719, 0.6757893741728115, 0.6756948383437322, 0.6757893741728115, 0.6757893741728115, 0.6757893741728115, 0.6758366420873511, 0.6757893741728115, 0.6758366420873511, 0.6758839100018907  …  0.6760729816600491, 0.6761202495745887, 0.6761675174891284, 0.6762620533182075, 0.676214785403668, 0.6762620533182075, 0.676214785403668, 0.6761202495745887, 0.6761675174891284, 0.6761675174891284], params = Dict{Any, Any}(\"b_1\" => [-0.00017304586875559972; -0.00015280176913301698; … ; -4.260440510533852e-5; -2.7521632064892113e-6;;], \"W_2\" => [0.407331208206197 0.4140792746837886 … 0.08185696553008814 0.00831224546855196], \"b_2\" => [-0.0013353720025043663;;], \"W_1\" => [0.29679696032415087 -0.2314229812927311 … -0.08628500912395891 -0.004565058254804369; 0.30088767477485984 -0.0353324138849462 … 0.32014955601492173 0.04674878849129097; … ; 0.06078031221483352 -0.11316928796395485 … 0.014890776511569104 0.11274294266418304; 0.006848393789434619 -0.12434890677730417 … -0.11352860172650522 0.004158478712077802]), updates = Any[42, 42, 42, 42, 42, 42, 42, 42, 42, 42  …  42, 42, 42, 42, 42, 42, 42, 42, 42, 42])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params_5,accuracy_train_5,accuracy_test_5,params_5,update_5=train_network([21, 21, 1], X_train_std, transpose(y_train), X_test_std, transpose(y_test); η=0.001, epochs=100, seed=18, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accessory-correspondence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration -> 1, test accuracy -> 0.675363962941955\n",
      "confusion matrix of test data:\n",
      "true_true_result->7827,  true_false_result->4117\n",
      "false_false_result->6461,   false_true_result->2751\n",
      "Iteration -> 11, test accuracy -> 0.6756948383437322\n",
      "confusion matrix of test data:\n",
      "true_true_result->7826,  true_false_result->4109\n",
      "false_false_result->6469,   false_true_result->2752\n",
      "Iteration -> 21, test accuracy -> 0.6756475704291927\n",
      "confusion matrix of test data:\n",
      "true_true_result->7820,  true_false_result->4104\n",
      "false_false_result->6474,   false_true_result->2758\n",
      "Iteration -> 31, test accuracy -> 0.6757893741728115\n",
      "confusion matrix of test data:\n",
      "true_true_result->7818,  true_false_result->4099\n",
      "false_false_result->6479,   false_true_result->2760\n",
      "Iteration -> 41, test accuracy -> 0.6758839100018907\n",
      "confusion matrix of test data:\n",
      "true_true_result->7816,  true_false_result->4095\n",
      "false_false_result->6483,   false_true_result->2762\n",
      "Iteration -> 51, test accuracy -> 0.6759784458309699\n",
      "confusion matrix of test data:\n",
      "true_true_result->7814,  true_false_result->4091\n",
      "false_false_result->6487,   false_true_result->2764\n",
      "Iteration -> 61, test accuracy -> 0.6763565891472868\n",
      "confusion matrix of test data:\n",
      "true_true_result->7812,  true_false_result->4081\n",
      "false_false_result->6497,   false_true_result->2766\n",
      "Iteration -> 71, test accuracy -> 0.6764983928909056\n",
      "confusion matrix of test data:\n",
      "true_true_result->7807,  true_false_result->4073\n",
      "false_false_result->6505,   false_true_result->2771\n",
      "Iteration -> 81, test accuracy -> 0.6765929287199849\n",
      "confusion matrix of test data:\n",
      "true_true_result->7806,  true_false_result->4070\n",
      "false_false_result->6508,   false_true_result->2772\n",
      "Iteration -> 91, test accuracy -> 0.6765929287199849\n",
      "confusion matrix of test data:\n",
      "true_true_result->7804,  true_false_result->4068\n",
      "false_false_result->6510,   false_true_result->2774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(initial_params = Dict{Any, Any}(\"b_1\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"W_2\" => [0.29302463740596874 -0.29121365611252276 … 0.33788571077716295 -0.40595596279813295], \"b_2\" => [0.0;;], \"W_1\" => [0.21207566253085172 0.24658533633820967 … 0.2883464435692282 0.03731727556467628; -0.2107649704981322 -0.16904743986149007 … -0.14649924597965489 -0.30136894103987516; … ; 0.24454372371937272 -0.031211608233788588 … -0.08090213775404789 0.14967322749210718; -0.29380935518226214 0.00501858160062785 … 0.12751286677824805 0.01202686364700849]), accuracy_train = Any[0.6780566702153796, 0.6780771435590861, 0.6780771435590861, 0.6780976169027926, 0.6780976169027926, 0.678118090246499, 0.678199983621325, 0.6782818769961511, 0.6783023503398575, 0.6783432970272705  …  0.6786708705265744, 0.6786708705265744, 0.6786913438702808, 0.6787322905576939, 0.6787527639014004, 0.6787732372451069, 0.6788346572762264, 0.6788346572762264, 0.6788756039636393, 0.6788960773073458], accuracy_test = Any[0.675363962941955, 0.675363962941955, 0.6754584987710343, 0.6754584987710343, 0.6755057666855738, 0.6755530346001134, 0.6755530346001134, 0.6756003025146531, 0.6756003025146531, 0.6756475704291927  …  0.6765929287199849, 0.6766401966345245, 0.6766874645490641, 0.6767347324636037, 0.6767820003781433, 0.676829268292683, 0.676829268292683, 0.676829268292683, 0.676829268292683, 0.676829268292683], params = Dict{Any, Any}(\"b_1\" => [-4.965340655494618e-5; 4.799791585486145e-5; … ; -8.359880484085407e-5; 7.755263996815956e-6;;], \"W_2\" => [0.29368046334070597 -0.2929769519078461 … 0.3382192557633199 -0.41063593969969353], \"b_2\" => [-0.0009337829947170785;;], \"W_1\" => [0.2127362914934548 0.247372324187373 … 0.2880481515531111 0.03690937055505562; -0.21141926564951133 -0.16983103461833787 … -0.1462144410051213 -0.3009449323078076; … ; 0.24532673342331543 -0.03027307825263614 … -0.0812153759218386 0.14918802240105886; -0.2944415316545491 0.0041203980560910215 … 0.1277414875654906 0.012432743417842264]), updates = Any[42, 42, 42, 42, 42, 42, 42, 42, 42, 42  …  42, 42, 42, 42, 42, 42, 42, 42, 42, 42])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params_6,accuracy_train_6,accuracy_test_6,params_6,update_6=train_network([21, 21, 1], X_train_std, transpose(y_train), X_test_std, transpose(y_test); η=0.001, epochs=100, seed=19, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aggressive-plaintiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration -> 1, test accuracy -> 0.5178672716959728\n",
      "confusion matrix of test data:\n",
      "true_true_result->8839,  true_false_result->8461\n",
      "false_false_result->2117,   false_true_result->1739\n",
      "Iteration -> 11, test accuracy -> 0.5214596332009832\n",
      "confusion matrix of test data:\n",
      "true_true_result->8817,  true_false_result->8363\n",
      "false_false_result->2215,   false_true_result->1761\n",
      "Iteration -> 21, test accuracy -> 0.5234448856116468\n",
      "confusion matrix of test data:\n",
      "true_true_result->8780,  true_false_result->8284\n",
      "false_false_result->2294,   false_true_result->1798\n",
      "Iteration -> 31, test accuracy -> 0.5254301380223104\n",
      "confusion matrix of test data:\n",
      "true_true_result->8746,  true_false_result->8208\n",
      "false_false_result->2370,   false_true_result->1832\n",
      "Iteration -> 41, test accuracy -> 0.528029873321989\n",
      "confusion matrix of test data:\n",
      "true_true_result->8706,  true_false_result->8113\n",
      "false_false_result->2465,   false_true_result->1872\n",
      "Iteration -> 51, test accuracy -> 0.5304405369635091\n",
      "confusion matrix of test data:\n",
      "true_true_result->8676,  true_false_result->8032\n",
      "false_false_result->2546,   false_true_result->1902\n",
      "Iteration -> 61, test accuracy -> 0.5335602193231235\n",
      "confusion matrix of test data:\n",
      "true_true_result->8650,  true_false_result->7940\n",
      "false_false_result->2638,   false_true_result->1928\n",
      "Iteration -> 71, test accuracy -> 0.5358290792210247\n",
      "confusion matrix of test data:\n",
      "true_true_result->8618,  true_false_result->7860\n",
      "false_false_result->2718,   false_true_result->1960\n",
      "Iteration -> 81, test accuracy -> 0.538239742862545\n",
      "confusion matrix of test data:\n",
      "true_true_result->8590,  true_false_result->7781\n",
      "false_false_result->2797,   false_true_result->1988\n",
      "Iteration -> 91, test accuracy -> 0.5417375685384761\n",
      "confusion matrix of test data:\n",
      "true_true_result->8574,  true_false_result->7691\n",
      "false_false_result->2887,   false_true_result->2004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(initial_params = Dict{Any, Any}(\"b_1\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"W_2\" => [-0.0897300417372711 0.4987670251221289 … -0.06276834912411437 0.11251144099470668], \"b_2\" => [0.0;;], \"W_1\" => [-0.06494183635483317 0.20956859707822845 … -0.3031345360943844 0.009308297608299291; 0.360981070526061 0.10852230306663063 … -0.2037027720693093 0.3767107178140272; … ; -0.04542839586564133 -0.24360755839719864 … -0.22643998883047428 -0.2805727441634521; 0.08142980263531663 0.020070701188358245 … -0.2708364627390524 0.1570474185921771]), accuracy_train = Any[0.5205142903939072, 0.520923757268037, 0.521087544017689, 0.5215379575792318, 0.5218655310785357, 0.5220907378593072, 0.5223364179837852, 0.5224387847023176, 0.5226435181393825, 0.5229710916386864  …  0.543669642125952, 0.5439562689378429, 0.5440586356563754, 0.5443452624682663, 0.5447752026861027, 0.5449594627794612, 0.545532716403243, 0.5459217099336664, 0.5461059700270248, 0.5463925968389157], accuracy_test = Any[0.5178672716959728, 0.5181508791832105, 0.5184817545849877, 0.519048969559463, 0.519663452448478, 0.5200888636793345, 0.5201833995084136, 0.5206088107392702, 0.5210342219701267, 0.5212232936282851  …  0.5417375685384761, 0.5417375685384761, 0.5422575155984118, 0.5423993193420307, 0.5421629797693326, 0.5422575155984118, 0.542352051427491, 0.5422575155984118, 0.542115711854793, 0.5421629797693326], params = Dict{Any, Any}(\"b_1\" => [0.00014837594770799456; -0.0008239067545648156; … ; 0.0001027613990817893; -0.00017475495580039966;;], \"W_2\" => [-0.094143783966523 0.49542664069899806 … -0.06670105414485564 0.10935945565533535], \"b_2\" => [-0.007419355325333698;;], \"W_1\" => [-0.06519932380271082 0.20933670433065502 … -0.3029760347832927 0.009547772019639014; 0.36236755062509124 0.10972550603773182 … -0.20449466024658544 0.37545204249703257; … ; -0.045607250064677377 -0.2437674802575071 … -0.22633327391462385 -0.28040260194644334; 0.08174390554466698 0.020349374651324127 … -0.27102216148883324 0.15675105333273678]), updates = Any[42, 42, 42, 42, 42, 42, 42, 42, 42, 42  …  42, 42, 42, 42, 42, 42, 42, 42, 42, 42])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params_7,accuracy_train_7,accuracy_test_7,params_7,update_7=train_network([21, 21, 1], X_train_std, transpose(y_train), X_test_std, transpose(y_test); η=0.001, epochs=100, seed=20, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "explicit-rebound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration -> 1, test accuracy -> 0.5894308943089431\n",
      "confusion matrix of test data:\n",
      "true_true_result->9505,  true_false_result->7613\n",
      "false_false_result->2965,   false_true_result->1073\n",
      "Iteration -> 11, test accuracy -> 0.5916997542068444\n",
      "confusion matrix of test data:\n",
      "true_true_result->9478,  true_false_result->7538\n",
      "false_false_result->3040,   false_true_result->1100\n",
      "Iteration -> 21, test accuracy -> 0.5936377387029684\n",
      "confusion matrix of test data:\n",
      "true_true_result->9447,  true_false_result->7466\n",
      "false_false_result->3112,   false_true_result->1131\n",
      "Iteration -> 31, test accuracy -> 0.5954339194554736\n",
      "confusion matrix of test data:\n",
      "true_true_result->9423,  true_false_result->7404\n",
      "false_false_result->3174,   false_true_result->1155\n",
      "Iteration -> 41, test accuracy -> 0.5974664397806769\n",
      "confusion matrix of test data:\n",
      "true_true_result->9398,  true_false_result->7336\n",
      "false_false_result->3242,   false_true_result->1180\n",
      "Iteration -> 51, test accuracy -> 0.5989790130459444\n",
      "confusion matrix of test data:\n",
      "true_true_result->9365,  true_false_result->7271\n",
      "false_false_result->3307,   false_true_result->1213\n",
      "Iteration -> 61, test accuracy -> 0.601578748345623\n",
      "confusion matrix of test data:\n",
      "true_true_result->9348,  true_false_result->7199\n",
      "false_false_result->3379,   false_true_result->1230\n",
      "Iteration -> 71, test accuracy -> 0.6034694649272074\n",
      "confusion matrix of test data:\n",
      "true_true_result->9322,  true_false_result->7133\n",
      "false_false_result->3445,   false_true_result->1256\n",
      "Iteration -> 81, test accuracy -> 0.6040366799016827\n",
      "confusion matrix of test data:\n",
      "true_true_result->9295,  true_false_result->7094\n",
      "false_false_result->3484,   false_true_result->1283\n",
      "Iteration -> 91, test accuracy -> 0.6056437889960294\n",
      "confusion matrix of test data:\n",
      "true_true_result->9272,  true_false_result->7037\n",
      "false_false_result->3541,   false_true_result->1306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(initial_params = Dict{Any, Any}(\"b_1\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"W_2\" => [-0.2710574054343566 0.031680103227317284 … -0.4350736670540867 0.4986928226090672], \"b_2\" => [0.0;;], \"W_1\" => [-0.19617694727062543 -0.08722963396698381 … 0.21471473159561466 -0.10342887634939532; 0.022928375376405238 0.23160394307126295 … -0.2511268950366402 -0.2254126303331562; … ; -0.3148832023376584 -0.26903875199041943 … 0.16587442131648616 -0.19282769725140456; 0.3609273666898979 -0.3415803969667988 … 0.25449766851306466 0.19087449629987643]), accuracy_train = Any[0.5899803455900418, 0.5903079190893457, 0.5904717058389977, 0.590860699369421, 0.590860699369421, 0.5910449594627795, 0.5911473261813119, 0.5914134796494963, 0.5916796331176808, 0.5918843665547457  …  0.6088977151748424, 0.6090819752682008, 0.6093890754237982, 0.6096552288919826, 0.6097985422979281, 0.610003275734993, 0.6100646957661126, 0.6103513225780034, 0.6105151093276554, 0.6108017361395464], accuracy_test = Any[0.5894308943089431, 0.589572698052562, 0.5896199659671015, 0.590045377197958, 0.5902817167706561, 0.5904707884288145, 0.5907071280015126, 0.5908016638305917, 0.5910380034032898, 0.5912743429759879  …  0.6056437889960294, 0.6059273964832672, 0.6062110039705049, 0.606305539799584, 0.6064946114577425, 0.6066364152013614, 0.6067782189449802, 0.6066836831159009, 0.6066836831159009, 0.606920022688599], params = Dict{Any, Any}(\"b_1\" => [0.0006514995581375577; -5.9598628836168597e-5; … ; 0.0009161829115213342; -0.0010616717675944222;;], \"W_2\" => [-0.27612351880498154 0.029893220263390023 … -0.4423913039815286 0.49274074130320017], \"b_2\" => [-0.010495814564412224;;], \"W_1\" => [-0.19684751131545494 -0.08797393064723062 … 0.21509436858256464 -0.10292010931773832; 0.02298518792295802 0.2316671861854199 … -0.25115102559487495 -0.22545034330177316; … ; -0.31576020962914325 -0.27003895572930203 … 0.1663638150266665 -0.19212083880527384; 0.36204822523012936 -0.34033710371313136 … 0.25388527456460563 0.19003265861727747]), updates = Any[42, 42, 42, 42, 42, 42, 42, 42, 42, 42  …  42, 42, 42, 42, 42, 42, 42, 42, 42, 42])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params_8,accuracy_train_8,accuracy_test_8,params_8,update_8=train_network([21, 21, 1], X_train_std, transpose(y_train), X_test_std, transpose(y_test); η=0.001, epochs=100, seed=23, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "raised-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration -> 1, test accuracy -> 0.5886746076763093\n",
      "confusion matrix of test data:\n",
      "true_true_result->10078,  true_false_result->8202\n",
      "false_false_result->2376,   false_true_result->500\n",
      "Iteration -> 11, test accuracy -> 0.5924560408394781\n",
      "confusion matrix of test data:\n",
      "true_true_result->10052,  true_false_result->8096\n",
      "false_false_result->2482,   false_true_result->526\n",
      "Iteration -> 21, test accuracy -> 0.5961902060881074\n",
      "confusion matrix of test data:\n",
      "true_true_result->10031,  true_false_result->7996\n",
      "false_false_result->2582,   false_true_result->547\n",
      "Iteration -> 31, test accuracy -> 0.5997352996785782\n",
      "confusion matrix of test data:\n",
      "true_true_result->10001,  true_false_result->7891\n",
      "false_false_result->2687,   false_true_result->577\n",
      "Iteration -> 41, test accuracy -> 0.602193231234638\n",
      "confusion matrix of test data:\n",
      "true_true_result->9969,  true_false_result->7807\n",
      "false_false_result->2771,   false_true_result->609\n",
      "Iteration -> 51, test accuracy -> 0.6047929665343165\n",
      "confusion matrix of test data:\n",
      "true_true_result->9946,  true_false_result->7729\n",
      "false_false_result->2849,   false_true_result->632\n",
      "Iteration -> 61, test accuracy -> 0.6075817734921535\n",
      "confusion matrix of test data:\n",
      "true_true_result->9914,  true_false_result->7638\n",
      "false_false_result->2940,   false_true_result->664\n",
      "Iteration -> 71, test accuracy -> 0.6096142938173568\n",
      "confusion matrix of test data:\n",
      "true_true_result->9884,  true_false_result->7565\n",
      "false_false_result->3013,   false_true_result->694\n",
      "Iteration -> 81, test accuracy -> 0.6123085649461146\n",
      "confusion matrix of test data:\n",
      "true_true_result->9856,  true_false_result->7480\n",
      "false_false_result->3098,   false_true_result->722\n",
      "Iteration -> 91, test accuracy -> 0.6140102098695406\n",
      "confusion matrix of test data:\n",
      "true_true_result->9833,  true_false_result->7421\n",
      "false_false_result->3157,   false_true_result->745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(initial_params = Dict{Any, Any}(\"b_1\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"W_2\" => [-0.5088915576818723 -0.2970392170606271 … -0.3173735607402656 0.12327250056949267], \"b_2\" => [0.0;;], \"W_1\" => [-0.3683086692202554 -0.31756583262355453 … 0.3045192441481498 0.05214091999423097; -0.21498120196801854 0.09664575935947044 … -0.14867123396583085 0.3506962878300454; … ; -0.22969811944692248 0.3637023694734296 … -0.27992738762128433 -0.028492736705016525; 0.08921808576078946 -0.32873356310294555 … -0.008650560468449645 0.20186601079752006]), accuracy_train = Any[0.5915158463680288, 0.5920481533043976, 0.5923962001474081, 0.592846613708951, 0.5930513471460159, 0.5935427073949717, 0.5939112275816887, 0.594300221112112, 0.594586847924003, 0.5950167881418393  …  0.6179059864056998, 0.6182130865612971, 0.6185816067480141, 0.618704446810253, 0.6189296535910245, 0.6193595938088609, 0.6195643272459258, 0.6197895340266972, 0.6200761608385882, 0.6203218409630661], accuracy_test = Any[0.5886746076763093, 0.588958215163547, 0.5894781622234827, 0.5899035734543392, 0.5901399130270373, 0.5906598600869729, 0.5910380034032898, 0.591510682548686, 0.5916052183777651, 0.5918888258650028  …  0.6140102098695406, 0.6141520136131594, 0.6144356211003971, 0.614813764416714, 0.6152391756475705, 0.6153337114766496, 0.6157591227075061, 0.6160427301947438, 0.6160427301947438, 0.6162318018529023], params = Dict{Any, Any}(\"b_1\" => [0.0014025819046310569; 0.0008402326371077134; … ; 0.0008774844412525695; -0.0003266828041506769;;], \"W_2\" => [-0.5161839165550532 -0.3032161249447943 … -0.3222476833370276 0.11510510603752426], \"b_2\" => [-0.012890764347044907;;], \"W_1\" => [-0.3693743267050771 -0.3186344848841023 … 0.30515357230786644 0.05298345350975534; -0.2156362601112321 0.09599089287123365 … -0.14825738264985847 0.35125248874271675; … ; -0.2303805351845314 0.36306161196686565 … -0.2795532176300129 -0.027982689587274057; 0.08946938940980492 -0.32850309359364205 … -0.00879930902778219 0.20167743456283435]), updates = Any[42, 42, 42, 42, 42, 42, 42, 42, 42, 42  …  42, 42, 42, 42, 42, 42, 42, 42, 42, 42])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params_9,accuracy_train_9,accuracy_test_9,params_9,update_9=train_network([21, 21, 1], X_train_std, transpose(y_train), X_test_std, transpose(y_test); η=0.001, epochs=100, seed=31, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "applicable-metabolism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration -> 1, test accuracy -> 0.6154755152202684\n",
      "confusion matrix of test data:\n",
      "true_true_result->7905,  true_false_result->5462\n",
      "false_false_result->5116,   false_true_result->2673\n",
      "Iteration -> 11, test accuracy -> 0.6160427301947438\n",
      "confusion matrix of test data:\n",
      "true_true_result->7888,  true_false_result->5433\n",
      "false_false_result->5145,   false_true_result->2690\n",
      "Iteration -> 21, test accuracy -> 0.6165154093401399\n",
      "confusion matrix of test data:\n",
      "true_true_result->7867,  true_false_result->5402\n",
      "false_false_result->5176,   false_true_result->2711\n",
      "Iteration -> 31, test accuracy -> 0.6173189638873133\n",
      "confusion matrix of test data:\n",
      "true_true_result->7852,  true_false_result->5370\n",
      "false_false_result->5208,   false_true_result->2726\n",
      "Iteration -> 41, test accuracy -> 0.6179334467763282\n",
      "confusion matrix of test data:\n",
      "true_true_result->7825,  true_false_result->5330\n",
      "false_false_result->5248,   false_true_result->2753\n",
      "Iteration -> 51, test accuracy -> 0.6188788050671205\n",
      "confusion matrix of test data:\n",
      "true_true_result->7812,  true_false_result->5297\n",
      "false_false_result->5281,   false_true_result->2766\n",
      "Iteration -> 61, test accuracy -> 0.6203441104178483\n",
      "confusion matrix of test data:\n",
      "true_true_result->7800,  true_false_result->5254\n",
      "false_false_result->5324,   false_true_result->2778\n",
      "Iteration -> 71, test accuracy -> 0.6205331820760068\n",
      "confusion matrix of test data:\n",
      "true_true_result->7784,  true_false_result->5234\n",
      "false_false_result->5344,   false_true_result->2794\n",
      "Iteration -> 81, test accuracy -> 0.6213367366231802\n",
      "confusion matrix of test data:\n",
      "true_true_result->7769,  true_false_result->5202\n",
      "false_false_result->5376,   false_true_result->2809\n",
      "Iteration -> 91, test accuracy -> 0.6217621478540367\n",
      "confusion matrix of test data:\n",
      "true_true_result->7749,  true_false_result->5173\n",
      "false_false_result->5405,   false_true_result->2829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(initial_params = Dict{Any, Any}(\"b_1\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"W_2\" => [0.2842773688008093 -0.44424798627009615 … 0.4995046203097001 -0.47579743602515506], \"b_2\" => [0.0;;], \"W_1\" => [0.2057448543053154 0.056705239310199926 … -0.05044816008398961 0.15934876087466865; -0.3215230871037613 -0.24586369233716796 … -0.33619911655800494 0.0895456350347261; … ; 0.36151490273030334 -0.34461633358265964 … 0.03258360657025195 0.13972642066731017; -0.3443569024392893 0.10742345296819944 … -0.07951893038410737 0.233853568429721]), accuracy_train = Any[0.6207517811809025, 0.620813201212022, 0.6209974613053804, 0.6210588813364999, 0.6211612480550324, 0.6212636147735648, 0.6213250348046843, 0.6214069281795103, 0.6213659814920973, 0.6213250348046843  …  0.6264433707313078, 0.6266890508557857, 0.6267299975431988, 0.6268733109491442, 0.6270985177299156, 0.6271189910736221, 0.6272827778232741, 0.6273032511669806, 0.6272827778232741, 0.6272213577921546], accuracy_test = Any[0.6154755152202684, 0.6155700510493477, 0.6157118547929665, 0.6158063906220458, 0.6158536585365854, 0.6157591227075061, 0.6158536585365854, 0.6159481943656646, 0.6159481943656646, 0.6159481943656646  …  0.6217621478540367, 0.6218094157685763, 0.6219039515976555, 0.6220457553412744, 0.6222820949139724, 0.6223766307430516, 0.6222820949139724, 0.622329362828512, 0.6222820949139724, 0.622329362828512], params = Dict{Any, Any}(\"b_1\" => [-0.00024514348182938734; 0.0003610990190835551; … ; -0.00044305956024078626; 0.00039317611146537774;;], \"W_2\" => [0.2817049243407819 -0.4455618806384436 … 0.4977237930316546 -0.4800767182142385], \"b_2\" => [-0.00381171881295616;;], \"W_1\" => [0.20647608899683123 0.05746488300448115 … -0.05092484796299631 0.15871742925246488; -0.3226686364415704 -0.24706056120245856 … -0.3354823749585029 0.09052058692529308; … ; 0.3628569342096289 -0.3432194010468884 … 0.031706328742682005 0.13854343752951243; -0.34548094416335584 0.10613350492392665 … -0.07872471404528053 0.2348762220740683]), updates = Any[42, 42, 42, 42, 42, 42, 42, 42, 42, 42  …  42, 42, 42, 42, 42, 42, 42, 42, 42, 42])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params_10,accuracy_train_10,accuracy_test_10,params_10,update_10=train_network([21, 21, 1], X_train_std, transpose(y_train), X_test_std, transpose(y_test); η=0.001, epochs=100, seed=25, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "grave-agent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Any}:\n",
       " 0.5008475964294484\n",
       " 0.5130353779379248\n",
       " 0.5103406764392762\n",
       " 0.6529023012038325\n",
       " 0.6787697567766763\n",
       " 0.6785119973794126\n",
       " 0.5330314880026205\n",
       " 0.6004780525755467\n",
       " 0.6065131848333469\n",
       " 0.6237908443206944"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_train=[]\n",
    "push!(accuracy_train,mean(accuracy_train_1))\n",
    "push!(accuracy_train,mean(accuracy_train_2))\n",
    "push!(accuracy_train,mean(accuracy_train_3))\n",
    "push!(accuracy_train,mean(accuracy_train_4))\n",
    "push!(accuracy_train,mean(accuracy_train_5))\n",
    "push!(accuracy_train,mean(accuracy_train_6))\n",
    "push!(accuracy_train,mean(accuracy_train_7))\n",
    "push!(accuracy_train,mean(accuracy_train_8))\n",
    "push!(accuracy_train,mean(accuracy_train_9))\n",
    "push!(accuracy_train,mean(accuracy_train_10))\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "amazing-natural",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Any}:\n",
       " 0.5005071847230098\n",
       " 0.5143037436188314\n",
       " 0.511842503308754\n",
       " 0.6467451314048026\n",
       " 0.6762190395159771\n",
       " 0.676113632066553\n",
       " 0.5306976744186046\n",
       " 0.5989875212705618\n",
       " 0.6040376252599735\n",
       " 0.6189095292115713"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_test=[]\n",
    "push!(accuracy_test,mean(accuracy_test_1))\n",
    "push!(accuracy_test,mean(accuracy_test_2))\n",
    "push!(accuracy_test,mean(accuracy_test_3))\n",
    "push!(accuracy_test,mean(accuracy_test_4))\n",
    "push!(accuracy_test,mean(accuracy_test_5))\n",
    "push!(accuracy_test,mean(accuracy_test_6))\n",
    "push!(accuracy_test,mean(accuracy_test_7))\n",
    "push!(accuracy_test,mean(accuracy_test_8))\n",
    "push!(accuracy_test,mean(accuracy_test_9))\n",
    "push!(accuracy_test,mean(accuracy_test_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "legendary-priest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Any}:\n",
       " 42.0\n",
       " 42.0\n",
       " 42.0\n",
       " 42.0\n",
       " 42.0\n",
       " 42.0\n",
       " 42.0\n",
       " 42.0\n",
       " 42.0\n",
       " 42.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update=[]\n",
    "push!(update,mean(update_1))\n",
    "push!(update,mean(update_2))\n",
    "push!(update,mean(update_3))\n",
    "push!(update,mean(update_4))\n",
    "push!(update,mean(update_5))\n",
    "push!(update,mean(update_6))\n",
    "push!(update,mean(update_7))\n",
    "push!(update,mean(update_8))\n",
    "push!(update,mean(update_9))\n",
    "push!(update,mean(update_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "collectible-violin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip250\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip250)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip251\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip250)\" d=\"\n",
       "M186.274 1486.45 L2352.76 1486.45 L2352.76 47.2441 L186.274 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip252\">\n",
       "    <rect x=\"186\" y=\"47\" width=\"2167\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  247.59,1486.45 247.59,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  758.552,1486.45 758.552,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1269.51,1486.45 1269.51,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1780.48,1486.45 1780.48,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2291.44,1486.45 2291.44,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  247.59,1486.45 247.59,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  758.552,1486.45 758.552,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1269.51,1486.45 1269.51,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1780.48,1486.45 1780.48,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2291.44,1486.45 2291.44,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip250)\" d=\"M198.064 1518.36 L186.259 1536.81 L198.064 1536.81 L198.064 1518.36 M196.837 1514.29 L202.717 1514.29 L202.717 1536.81 L207.648 1536.81 L207.648 1540.7 L202.717 1540.7 L202.717 1548.85 L198.064 1548.85 L198.064 1540.7 L182.463 1540.7 L182.463 1536.19 L196.837 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M219.407 1544.91 L235.726 1544.91 L235.726 1548.85 L213.782 1548.85 L213.782 1544.91 Q216.444 1542.16 221.027 1537.53 Q225.634 1532.88 226.814 1531.53 Q229.06 1529.01 229.939 1527.27 Q230.842 1525.51 230.842 1523.82 Q230.842 1521.07 228.897 1519.33 Q226.976 1517.6 223.874 1517.6 Q221.675 1517.6 219.222 1518.36 Q216.791 1519.13 214.013 1520.68 L214.013 1515.95 Q216.837 1514.82 219.291 1514.24 Q221.745 1513.66 223.782 1513.66 Q229.152 1513.66 232.347 1516.35 Q235.541 1519.03 235.541 1523.52 Q235.541 1525.65 234.731 1527.57 Q233.944 1529.47 231.837 1532.07 Q231.259 1532.74 228.157 1535.95 Q225.055 1539.15 219.407 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M245.541 1542.97 L250.425 1542.97 L250.425 1548.85 L245.541 1548.85 L245.541 1542.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M270.61 1517.37 Q266.999 1517.37 265.17 1520.93 Q263.365 1524.47 263.365 1531.6 Q263.365 1538.71 265.17 1542.27 Q266.999 1545.82 270.61 1545.82 Q274.244 1545.82 276.05 1542.27 Q277.879 1538.71 277.879 1531.6 Q277.879 1524.47 276.05 1520.93 Q274.244 1517.37 270.61 1517.37 M270.61 1513.66 Q276.42 1513.66 279.476 1518.27 Q282.555 1522.85 282.555 1531.6 Q282.555 1540.33 279.476 1544.94 Q276.42 1549.52 270.61 1549.52 Q264.8 1549.52 261.721 1544.94 Q258.666 1540.33 258.666 1531.6 Q258.666 1522.85 261.721 1518.27 Q264.8 1513.66 270.61 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M300.772 1517.37 Q297.161 1517.37 295.332 1520.93 Q293.527 1524.47 293.527 1531.6 Q293.527 1538.71 295.332 1542.27 Q297.161 1545.82 300.772 1545.82 Q304.406 1545.82 306.212 1542.27 Q308.041 1538.71 308.041 1531.6 Q308.041 1524.47 306.212 1520.93 Q304.406 1517.37 300.772 1517.37 M300.772 1513.66 Q306.582 1513.66 309.638 1518.27 Q312.716 1522.85 312.716 1531.6 Q312.716 1540.33 309.638 1544.94 Q306.582 1549.52 300.772 1549.52 Q294.962 1549.52 291.883 1544.94 Q288.828 1540.33 288.828 1531.6 Q288.828 1522.85 291.883 1518.27 Q294.962 1513.66 300.772 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M709.525 1518.36 L697.719 1536.81 L709.525 1536.81 L709.525 1518.36 M708.298 1514.29 L714.177 1514.29 L714.177 1536.81 L719.108 1536.81 L719.108 1540.7 L714.177 1540.7 L714.177 1548.85 L709.525 1548.85 L709.525 1540.7 L693.923 1540.7 L693.923 1536.19 L708.298 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M730.867 1544.91 L747.187 1544.91 L747.187 1548.85 L725.242 1548.85 L725.242 1544.91 Q727.904 1542.16 732.488 1537.53 Q737.094 1532.88 738.275 1531.53 Q740.52 1529.01 741.4 1527.27 Q742.302 1525.51 742.302 1523.82 Q742.302 1521.07 740.358 1519.33 Q738.437 1517.6 735.335 1517.6 Q733.136 1517.6 730.682 1518.36 Q728.251 1519.13 725.474 1520.68 L725.474 1515.95 Q728.298 1514.82 730.751 1514.24 Q733.205 1513.66 735.242 1513.66 Q740.613 1513.66 743.807 1516.35 Q747.001 1519.03 747.001 1523.52 Q747.001 1525.65 746.191 1527.57 Q745.404 1529.47 743.298 1532.07 Q742.719 1532.74 739.617 1535.95 Q736.515 1539.15 730.867 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M757.001 1542.97 L761.886 1542.97 L761.886 1548.85 L757.001 1548.85 L757.001 1542.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M776.098 1544.91 L792.418 1544.91 L792.418 1548.85 L770.473 1548.85 L770.473 1544.91 Q773.135 1542.16 777.719 1537.53 Q782.325 1532.88 783.506 1531.53 Q785.751 1529.01 786.631 1527.27 Q787.534 1525.51 787.534 1523.82 Q787.534 1521.07 785.589 1519.33 Q783.668 1517.6 780.566 1517.6 Q778.367 1517.6 775.913 1518.36 Q773.483 1519.13 770.705 1520.68 L770.705 1515.95 Q773.529 1514.82 775.983 1514.24 Q778.436 1513.66 780.473 1513.66 Q785.844 1513.66 789.038 1516.35 Q792.233 1519.03 792.233 1523.52 Q792.233 1525.65 791.422 1527.57 Q790.635 1529.47 788.529 1532.07 Q787.95 1532.74 784.848 1535.95 Q781.747 1539.15 776.098 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M802.279 1514.29 L820.635 1514.29 L820.635 1518.22 L806.561 1518.22 L806.561 1526.7 Q807.58 1526.35 808.598 1526.19 Q809.617 1526 810.635 1526 Q816.422 1526 819.802 1529.17 Q823.182 1532.34 823.182 1537.76 Q823.182 1543.34 819.709 1546.44 Q816.237 1549.52 809.918 1549.52 Q807.742 1549.52 805.473 1549.15 Q803.228 1548.78 800.821 1548.04 L800.821 1543.34 Q802.904 1544.47 805.126 1545.03 Q807.348 1545.58 809.825 1545.58 Q813.83 1545.58 816.168 1543.48 Q818.506 1541.37 818.506 1537.76 Q818.506 1534.15 816.168 1532.04 Q813.83 1529.94 809.825 1529.94 Q807.95 1529.94 806.075 1530.35 Q804.223 1530.77 802.279 1531.65 L802.279 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1219.99 1518.36 L1208.18 1536.81 L1219.99 1536.81 L1219.99 1518.36 M1218.76 1514.29 L1224.64 1514.29 L1224.64 1536.81 L1229.57 1536.81 L1229.57 1540.7 L1224.64 1540.7 L1224.64 1548.85 L1219.99 1548.85 L1219.99 1540.7 L1204.39 1540.7 L1204.39 1536.19 L1218.76 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1241.33 1544.91 L1257.65 1544.91 L1257.65 1548.85 L1235.71 1548.85 L1235.71 1544.91 Q1238.37 1542.16 1242.95 1537.53 Q1247.56 1532.88 1248.74 1531.53 Q1250.98 1529.01 1251.86 1527.27 Q1252.77 1525.51 1252.77 1523.82 Q1252.77 1521.07 1250.82 1519.33 Q1248.9 1517.6 1245.8 1517.6 Q1243.6 1517.6 1241.15 1518.36 Q1238.72 1519.13 1235.94 1520.68 L1235.94 1515.95 Q1238.76 1514.82 1241.22 1514.24 Q1243.67 1513.66 1245.71 1513.66 Q1251.08 1513.66 1254.27 1516.35 Q1257.47 1519.03 1257.47 1523.52 Q1257.47 1525.65 1256.66 1527.57 Q1255.87 1529.47 1253.76 1532.07 Q1253.18 1532.74 1250.08 1535.95 Q1246.98 1539.15 1241.33 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1267.47 1542.97 L1272.35 1542.97 L1272.35 1548.85 L1267.47 1548.85 L1267.47 1542.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1282.58 1514.29 L1300.94 1514.29 L1300.94 1518.22 L1286.86 1518.22 L1286.86 1526.7 Q1287.88 1526.35 1288.9 1526.19 Q1289.92 1526 1290.94 1526 Q1296.73 1526 1300.11 1529.17 Q1303.48 1532.34 1303.48 1537.76 Q1303.48 1543.34 1300.01 1546.44 Q1296.54 1549.52 1290.22 1549.52 Q1288.04 1549.52 1285.78 1549.15 Q1283.53 1548.78 1281.12 1548.04 L1281.12 1543.34 Q1283.21 1544.47 1285.43 1545.03 Q1287.65 1545.58 1290.13 1545.58 Q1294.13 1545.58 1296.47 1543.48 Q1298.81 1541.37 1298.81 1537.76 Q1298.81 1534.15 1296.47 1532.04 Q1294.13 1529.94 1290.13 1529.94 Q1288.25 1529.94 1286.38 1530.35 Q1284.53 1530.77 1282.58 1531.65 L1282.58 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1322.7 1517.37 Q1319.09 1517.37 1317.26 1520.93 Q1315.45 1524.47 1315.45 1531.6 Q1315.45 1538.71 1317.26 1542.27 Q1319.09 1545.82 1322.7 1545.82 Q1326.33 1545.82 1328.14 1542.27 Q1329.97 1538.71 1329.97 1531.6 Q1329.97 1524.47 1328.14 1520.93 Q1326.33 1517.37 1322.7 1517.37 M1322.7 1513.66 Q1328.51 1513.66 1331.56 1518.27 Q1334.64 1522.85 1334.64 1531.6 Q1334.64 1540.33 1331.56 1544.94 Q1328.51 1549.52 1322.7 1549.52 Q1316.89 1549.52 1313.81 1544.94 Q1310.75 1540.33 1310.75 1531.6 Q1310.75 1522.85 1313.81 1518.27 Q1316.89 1513.66 1322.7 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1731.45 1518.36 L1719.64 1536.81 L1731.45 1536.81 L1731.45 1518.36 M1730.22 1514.29 L1736.1 1514.29 L1736.1 1536.81 L1741.03 1536.81 L1741.03 1540.7 L1736.1 1540.7 L1736.1 1548.85 L1731.45 1548.85 L1731.45 1540.7 L1715.85 1540.7 L1715.85 1536.19 L1730.22 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1752.79 1544.91 L1769.11 1544.91 L1769.11 1548.85 L1747.17 1548.85 L1747.17 1544.91 Q1749.83 1542.16 1754.41 1537.53 Q1759.02 1532.88 1760.2 1531.53 Q1762.45 1529.01 1763.32 1527.27 Q1764.23 1525.51 1764.23 1523.82 Q1764.23 1521.07 1762.28 1519.33 Q1760.36 1517.6 1757.26 1517.6 Q1755.06 1517.6 1752.61 1518.36 Q1750.18 1519.13 1747.4 1520.68 L1747.4 1515.95 Q1750.22 1514.82 1752.68 1514.24 Q1755.13 1513.66 1757.17 1513.66 Q1762.54 1513.66 1765.73 1516.35 Q1768.93 1519.03 1768.93 1523.52 Q1768.93 1525.65 1768.12 1527.57 Q1767.33 1529.47 1765.22 1532.07 Q1764.64 1532.74 1761.54 1535.95 Q1758.44 1539.15 1752.79 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1778.93 1542.97 L1783.81 1542.97 L1783.81 1548.85 L1778.93 1548.85 L1778.93 1542.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1792.82 1514.29 L1815.04 1514.29 L1815.04 1516.28 L1802.49 1548.85 L1797.61 1548.85 L1809.41 1518.22 L1792.82 1518.22 L1792.82 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M1824.2 1514.29 L1842.56 1514.29 L1842.56 1518.22 L1828.49 1518.22 L1828.49 1526.7 Q1829.51 1526.35 1830.52 1526.19 Q1831.54 1526 1832.56 1526 Q1838.35 1526 1841.73 1529.17 Q1845.11 1532.34 1845.11 1537.76 Q1845.11 1543.34 1841.63 1546.44 Q1838.16 1549.52 1831.84 1549.52 Q1829.67 1549.52 1827.4 1549.15 Q1825.15 1548.78 1822.75 1548.04 L1822.75 1543.34 Q1824.83 1544.47 1827.05 1545.03 Q1829.27 1545.58 1831.75 1545.58 Q1835.76 1545.58 1838.09 1543.48 Q1840.43 1541.37 1840.43 1537.76 Q1840.43 1534.15 1838.09 1532.04 Q1835.76 1529.94 1831.75 1529.94 Q1829.88 1529.94 1828 1530.35 Q1826.15 1530.77 1824.2 1531.65 L1824.2 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2241.92 1518.36 L2230.11 1536.81 L2241.92 1536.81 L2241.92 1518.36 M2240.69 1514.29 L2246.57 1514.29 L2246.57 1536.81 L2251.5 1536.81 L2251.5 1540.7 L2246.57 1540.7 L2246.57 1548.85 L2241.92 1548.85 L2241.92 1540.7 L2226.31 1540.7 L2226.31 1536.19 L2240.69 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2273.4 1530.21 Q2276.75 1530.93 2278.63 1533.2 Q2280.53 1535.47 2280.53 1538.8 Q2280.53 1543.92 2277.01 1546.72 Q2273.49 1549.52 2267.01 1549.52 Q2264.83 1549.52 2262.52 1549.08 Q2260.23 1548.66 2257.77 1547.81 L2257.77 1543.29 Q2259.72 1544.43 2262.03 1545.01 Q2264.35 1545.58 2266.87 1545.58 Q2271.27 1545.58 2273.56 1543.85 Q2275.87 1542.11 2275.87 1538.8 Q2275.87 1535.75 2273.72 1534.03 Q2271.59 1532.3 2267.77 1532.3 L2263.74 1532.3 L2263.74 1528.45 L2267.96 1528.45 Q2271.41 1528.45 2273.23 1527.09 Q2275.06 1525.7 2275.06 1523.11 Q2275.06 1520.45 2273.17 1519.03 Q2271.29 1517.6 2267.77 1517.6 Q2265.85 1517.6 2263.65 1518.01 Q2261.45 1518.43 2258.81 1519.31 L2258.81 1515.14 Q2261.48 1514.4 2263.79 1514.03 Q2266.13 1513.66 2268.19 1513.66 Q2273.51 1513.66 2276.61 1516.09 Q2279.72 1518.5 2279.72 1522.62 Q2279.72 1525.49 2278.07 1527.48 Q2276.43 1529.45 2273.4 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2289.39 1542.97 L2294.28 1542.97 L2294.28 1548.85 L2289.39 1548.85 L2289.39 1542.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2314.46 1517.37 Q2310.85 1517.37 2309.02 1520.93 Q2307.22 1524.47 2307.22 1531.6 Q2307.22 1538.71 2309.02 1542.27 Q2310.85 1545.82 2314.46 1545.82 Q2318.1 1545.82 2319.9 1542.27 Q2321.73 1538.71 2321.73 1531.6 Q2321.73 1524.47 2319.9 1520.93 Q2318.1 1517.37 2314.46 1517.37 M2314.46 1513.66 Q2320.27 1513.66 2323.33 1518.27 Q2326.41 1522.85 2326.41 1531.6 Q2326.41 1540.33 2323.33 1544.94 Q2320.27 1549.52 2314.46 1549.52 Q2308.65 1549.52 2305.57 1544.94 Q2302.52 1540.33 2302.52 1531.6 Q2302.52 1522.85 2305.57 1518.27 Q2308.65 1513.66 2314.46 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2344.62 1517.37 Q2341.01 1517.37 2339.18 1520.93 Q2337.38 1524.47 2337.38 1531.6 Q2337.38 1538.71 2339.18 1542.27 Q2341.01 1545.82 2344.62 1545.82 Q2348.26 1545.82 2350.06 1542.27 Q2351.89 1538.71 2351.89 1531.6 Q2351.89 1524.47 2350.06 1520.93 Q2348.26 1517.37 2344.62 1517.37 M2344.62 1513.66 Q2350.43 1513.66 2353.49 1518.27 Q2356.57 1522.85 2356.57 1531.6 Q2356.57 1540.33 2353.49 1544.94 Q2350.43 1549.52 2344.62 1549.52 Q2338.81 1549.52 2335.73 1544.94 Q2332.68 1540.33 2332.68 1531.6 Q2332.68 1522.85 2335.73 1518.27 Q2338.81 1513.66 2344.62 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  186.274,1449.58 2352.76,1449.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  186.274,1068.75 2352.76,1068.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  186.274,687.927 2352.76,687.927 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  186.274,307.102 2352.76,307.102 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,1486.45 186.274,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,1449.58 205.172,1449.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,1068.75 205.172,1068.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,687.927 205.172,687.927 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  186.274,307.102 205.172,307.102 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip250)\" d=\"M62.9365 1435.38 Q59.3254 1435.38 57.4967 1438.94 Q55.6912 1442.48 55.6912 1449.61 Q55.6912 1456.72 57.4967 1460.28 Q59.3254 1463.83 62.9365 1463.83 Q66.5707 1463.83 68.3763 1460.28 Q70.205 1456.72 70.205 1449.61 Q70.205 1442.48 68.3763 1438.94 Q66.5707 1435.38 62.9365 1435.38 M62.9365 1431.67 Q68.7467 1431.67 71.8022 1436.28 Q74.8809 1440.86 74.8809 1449.61 Q74.8809 1458.34 71.8022 1462.95 Q68.7467 1467.53 62.9365 1467.53 Q57.1264 1467.53 54.0477 1462.95 Q50.9921 1458.34 50.9921 1449.61 Q50.9921 1440.86 54.0477 1436.28 Q57.1264 1431.67 62.9365 1431.67 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M83.0984 1460.98 L87.9827 1460.98 L87.9827 1466.86 L83.0984 1466.86 L83.0984 1460.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M98.2141 1432.3 L116.57 1432.3 L116.57 1436.23 L102.496 1436.23 L102.496 1444.71 Q103.515 1444.36 104.534 1444.2 Q105.552 1444.01 106.571 1444.01 Q112.358 1444.01 115.737 1447.18 Q119.117 1450.35 119.117 1455.77 Q119.117 1461.35 115.645 1464.45 Q112.172 1467.53 105.853 1467.53 Q103.677 1467.53 101.409 1467.16 Q99.1632 1466.79 96.7558 1466.05 L96.7558 1461.35 Q98.8391 1462.48 101.061 1463.04 Q103.284 1463.59 105.76 1463.59 Q109.765 1463.59 112.103 1461.49 Q114.441 1459.38 114.441 1455.77 Q114.441 1452.16 112.103 1450.05 Q109.765 1447.95 105.76 1447.95 Q103.885 1447.95 102.01 1448.36 Q100.159 1448.78 98.2141 1449.66 L98.2141 1432.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M138.33 1435.38 Q134.719 1435.38 132.89 1438.94 Q131.084 1442.48 131.084 1449.61 Q131.084 1456.72 132.89 1460.28 Q134.719 1463.83 138.33 1463.83 Q141.964 1463.83 143.769 1460.28 Q145.598 1456.72 145.598 1449.61 Q145.598 1442.48 143.769 1438.94 Q141.964 1435.38 138.33 1435.38 M138.33 1431.67 Q144.14 1431.67 147.195 1436.28 Q150.274 1440.86 150.274 1449.61 Q150.274 1458.34 147.195 1462.95 Q144.14 1467.53 138.33 1467.53 Q132.519 1467.53 129.441 1462.95 Q126.385 1458.34 126.385 1449.61 Q126.385 1440.86 129.441 1436.28 Q132.519 1431.67 138.33 1431.67 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M63.9319 1054.55 Q60.3208 1054.55 58.4921 1058.12 Q56.6865 1061.66 56.6865 1068.79 Q56.6865 1075.89 58.4921 1079.46 Q60.3208 1083 63.9319 1083 Q67.5661 1083 69.3717 1079.46 Q71.2004 1075.89 71.2004 1068.79 Q71.2004 1061.66 69.3717 1058.12 Q67.5661 1054.55 63.9319 1054.55 M63.9319 1050.85 Q69.742 1050.85 72.7976 1055.45 Q75.8763 1060.04 75.8763 1068.79 Q75.8763 1077.51 72.7976 1082.12 Q69.742 1086.7 63.9319 1086.7 Q58.1217 1086.7 55.043 1082.12 Q51.9875 1077.51 51.9875 1068.79 Q51.9875 1060.04 55.043 1055.45 Q58.1217 1050.85 63.9319 1050.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M84.0938 1080.15 L88.978 1080.15 L88.978 1086.03 L84.0938 1086.03 L84.0938 1080.15 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M99.2095 1051.47 L117.566 1051.47 L117.566 1055.41 L103.492 1055.41 L103.492 1063.88 Q104.51 1063.53 105.529 1063.37 Q106.547 1063.19 107.566 1063.19 Q113.353 1063.19 116.733 1066.36 Q120.112 1069.53 120.112 1074.95 Q120.112 1080.52 116.64 1083.63 Q113.168 1086.7 106.848 1086.7 Q104.672 1086.7 102.404 1086.33 Q100.159 1085.96 97.7511 1085.22 L97.7511 1080.52 Q99.8345 1081.66 102.057 1082.21 Q104.279 1082.77 106.756 1082.77 Q110.76 1082.77 113.098 1080.66 Q115.436 1078.56 115.436 1074.95 Q115.436 1071.33 113.098 1069.23 Q110.76 1067.12 106.756 1067.12 Q104.881 1067.12 103.006 1067.54 Q101.154 1067.95 99.2095 1068.83 L99.2095 1051.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M129.371 1051.47 L147.728 1051.47 L147.728 1055.41 L133.654 1055.41 L133.654 1063.88 Q134.672 1063.53 135.691 1063.37 Q136.709 1063.19 137.728 1063.19 Q143.515 1063.19 146.894 1066.36 Q150.274 1069.53 150.274 1074.95 Q150.274 1080.52 146.802 1083.63 Q143.33 1086.7 137.01 1086.7 Q134.834 1086.7 132.566 1086.33 Q130.32 1085.96 127.913 1085.22 L127.913 1080.52 Q129.996 1081.66 132.219 1082.21 Q134.441 1082.77 136.918 1082.77 Q140.922 1082.77 143.26 1080.66 Q145.598 1078.56 145.598 1074.95 Q145.598 1071.33 143.26 1069.23 Q140.922 1067.12 136.918 1067.12 Q135.043 1067.12 133.168 1067.54 Q131.316 1067.95 129.371 1068.83 L129.371 1051.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M62.9365 673.726 Q59.3254 673.726 57.4967 677.291 Q55.6912 680.832 55.6912 687.962 Q55.6912 695.068 57.4967 698.633 Q59.3254 702.175 62.9365 702.175 Q66.5707 702.175 68.3763 698.633 Q70.205 695.068 70.205 687.962 Q70.205 680.832 68.3763 677.291 Q66.5707 673.726 62.9365 673.726 M62.9365 670.022 Q68.7467 670.022 71.8022 674.629 Q74.8809 679.212 74.8809 687.962 Q74.8809 696.689 71.8022 701.295 Q68.7467 705.879 62.9365 705.879 Q57.1264 705.879 54.0477 701.295 Q50.9921 696.689 50.9921 687.962 Q50.9921 679.212 54.0477 674.629 Q57.1264 670.022 62.9365 670.022 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M83.0984 699.328 L87.9827 699.328 L87.9827 705.207 L83.0984 705.207 L83.0984 699.328 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M108.746 686.064 Q105.598 686.064 103.746 688.217 Q101.918 690.369 101.918 694.119 Q101.918 697.846 103.746 700.022 Q105.598 702.175 108.746 702.175 Q111.895 702.175 113.723 700.022 Q115.575 697.846 115.575 694.119 Q115.575 690.369 113.723 688.217 Q111.895 686.064 108.746 686.064 M118.029 671.411 L118.029 675.67 Q116.27 674.837 114.464 674.397 Q112.682 673.957 110.922 673.957 Q106.293 673.957 103.839 677.082 Q101.409 680.207 101.061 686.527 Q102.427 684.513 104.487 683.448 Q106.547 682.36 109.024 682.36 Q114.233 682.36 117.242 685.531 Q120.274 688.68 120.274 694.119 Q120.274 699.443 117.126 702.661 Q113.978 705.879 108.746 705.879 Q102.751 705.879 99.5798 701.295 Q96.4085 696.689 96.4085 687.962 Q96.4085 679.768 100.297 674.906 Q104.186 670.022 110.737 670.022 Q112.496 670.022 114.279 670.369 Q116.084 670.717 118.029 671.411 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M138.33 673.726 Q134.719 673.726 132.89 677.291 Q131.084 680.832 131.084 687.962 Q131.084 695.068 132.89 698.633 Q134.719 702.175 138.33 702.175 Q141.964 702.175 143.769 698.633 Q145.598 695.068 145.598 687.962 Q145.598 680.832 143.769 677.291 Q141.964 673.726 138.33 673.726 M138.33 670.022 Q144.14 670.022 147.195 674.629 Q150.274 679.212 150.274 687.962 Q150.274 696.689 147.195 701.295 Q144.14 705.879 138.33 705.879 Q132.519 705.879 129.441 701.295 Q126.385 696.689 126.385 687.962 Q126.385 679.212 129.441 674.629 Q132.519 670.022 138.33 670.022 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M63.9319 292.9 Q60.3208 292.9 58.4921 296.465 Q56.6865 300.007 56.6865 307.136 Q56.6865 314.243 58.4921 317.807 Q60.3208 321.349 63.9319 321.349 Q67.5661 321.349 69.3717 317.807 Q71.2004 314.243 71.2004 307.136 Q71.2004 300.007 69.3717 296.465 Q67.5661 292.9 63.9319 292.9 M63.9319 289.197 Q69.742 289.197 72.7976 293.803 Q75.8763 298.386 75.8763 307.136 Q75.8763 315.863 72.7976 320.47 Q69.742 325.053 63.9319 325.053 Q58.1217 325.053 55.043 320.47 Q51.9875 315.863 51.9875 307.136 Q51.9875 298.386 55.043 293.803 Q58.1217 289.197 63.9319 289.197 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M84.0938 318.502 L88.978 318.502 L88.978 324.382 L84.0938 324.382 L84.0938 318.502 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M109.742 305.238 Q106.594 305.238 104.742 307.391 Q102.913 309.544 102.913 313.294 Q102.913 317.02 104.742 319.196 Q106.594 321.349 109.742 321.349 Q112.89 321.349 114.719 319.196 Q116.57 317.02 116.57 313.294 Q116.57 309.544 114.719 307.391 Q112.89 305.238 109.742 305.238 M119.024 290.585 L119.024 294.845 Q117.265 294.011 115.459 293.572 Q113.677 293.132 111.918 293.132 Q107.288 293.132 104.834 296.257 Q102.404 299.382 102.057 305.701 Q103.422 303.687 105.483 302.622 Q107.543 301.534 110.02 301.534 Q115.228 301.534 118.237 304.706 Q121.27 307.854 121.27 313.294 Q121.27 318.618 118.121 321.835 Q114.973 325.053 109.742 325.053 Q103.746 325.053 100.575 320.47 Q97.4039 315.863 97.4039 307.136 Q97.4039 298.942 101.293 294.081 Q105.182 289.197 111.733 289.197 Q113.492 289.197 115.274 289.544 Q117.08 289.891 119.024 290.585 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M129.371 289.822 L147.728 289.822 L147.728 293.757 L133.654 293.757 L133.654 302.229 Q134.672 301.882 135.691 301.72 Q136.709 301.534 137.728 301.534 Q143.515 301.534 146.894 304.706 Q150.274 307.877 150.274 313.294 Q150.274 318.872 146.802 321.974 Q143.33 325.053 137.01 325.053 Q134.834 325.053 132.566 324.682 Q130.32 324.312 127.913 323.571 L127.913 318.872 Q129.996 320.007 132.219 320.562 Q134.441 321.118 136.918 321.118 Q140.922 321.118 143.26 319.011 Q145.598 316.905 145.598 313.294 Q145.598 309.683 143.26 307.576 Q140.922 305.47 136.918 305.47 Q135.043 305.47 133.168 305.886 Q131.316 306.303 129.371 307.183 L129.371 289.822 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip252)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  247.59,1443.12 247.59,1350.29 247.59,1370.82 247.59,284.996 247.59,87.9763 247.59,89.9395 247.59,1197.99 247.59,684.286 247.59,638.319 247.59,506.724 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip252)\" style=\"stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  247.59,1445.72 247.59,1340.63 247.59,1359.38 247.59,331.892 247.59,107.404 247.59,108.207 247.59,1215.77 247.59,695.639 247.59,657.175 247.59,543.903 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip250)\" d=\"\n",
       "M1935.49 250.738 L2280.54 250.738 L2280.54 95.2176 L1935.49 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1935.49,250.738 2280.54,250.738 2280.54,95.2176 1935.49,95.2176 1935.49,250.738 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip250)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1959.49,147.058 2103.49,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip250)\" d=\"M2134.9 131.051 L2134.9 138.412 L2143.67 138.412 L2143.67 141.722 L2134.9 141.722 L2134.9 155.796 Q2134.9 158.967 2135.76 159.87 Q2136.64 160.773 2139.3 160.773 L2143.67 160.773 L2143.67 164.338 L2139.3 164.338 Q2134.37 164.338 2132.49 162.509 Q2130.62 160.657 2130.62 155.796 L2130.62 141.722 L2127.49 141.722 L2127.49 138.412 L2130.62 138.412 L2130.62 131.051 L2134.9 131.051 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2164.3 142.393 Q2163.58 141.977 2162.73 141.791 Q2161.89 141.583 2160.87 141.583 Q2157.26 141.583 2155.32 143.944 Q2153.4 146.282 2153.4 150.68 L2153.4 164.338 L2149.11 164.338 L2149.11 138.412 L2153.4 138.412 L2153.4 142.44 Q2154.74 140.078 2156.89 138.944 Q2159.04 137.787 2162.12 137.787 Q2162.56 137.787 2163.1 137.856 Q2163.63 137.903 2164.28 138.018 L2164.3 142.393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2180.55 151.305 Q2175.39 151.305 2173.4 152.486 Q2171.41 153.666 2171.41 156.514 Q2171.41 158.782 2172.89 160.125 Q2174.39 161.444 2176.96 161.444 Q2180.5 161.444 2182.63 158.944 Q2184.79 156.421 2184.79 152.254 L2184.79 151.305 L2180.55 151.305 M2189.04 149.546 L2189.04 164.338 L2184.79 164.338 L2184.79 160.402 Q2183.33 162.763 2181.15 163.898 Q2178.98 165.009 2175.83 165.009 Q2171.85 165.009 2169.48 162.787 Q2167.15 160.541 2167.15 156.791 Q2167.15 152.416 2170.06 150.194 Q2173 147.972 2178.81 147.972 L2184.79 147.972 L2184.79 147.555 Q2184.79 144.615 2182.84 143.018 Q2180.92 141.398 2177.42 141.398 Q2175.2 141.398 2173.1 141.93 Q2170.99 142.463 2169.04 143.527 L2169.04 139.592 Q2171.38 138.69 2173.58 138.25 Q2175.78 137.787 2177.86 137.787 Q2183.49 137.787 2186.27 140.703 Q2189.04 143.62 2189.04 149.546 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2197.82 138.412 L2202.08 138.412 L2202.08 164.338 L2197.82 164.338 L2197.82 138.412 M2197.82 128.319 L2202.08 128.319 L2202.08 133.713 L2197.82 133.713 L2197.82 128.319 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2232.54 148.689 L2232.54 164.338 L2228.28 164.338 L2228.28 148.828 Q2228.28 145.148 2226.85 143.319 Q2225.41 141.49 2222.54 141.49 Q2219.09 141.49 2217.1 143.69 Q2215.11 145.889 2215.11 149.685 L2215.11 164.338 L2210.83 164.338 L2210.83 138.412 L2215.11 138.412 L2215.11 142.44 Q2216.64 140.102 2218.7 138.944 Q2220.78 137.787 2223.49 137.787 Q2227.96 137.787 2230.25 140.565 Q2232.54 143.319 2232.54 148.689 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip250)\" style=\"stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1959.49,198.898 2103.49,198.898 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip250)\" d=\"M2134.9 182.891 L2134.9 190.252 L2143.67 190.252 L2143.67 193.562 L2134.9 193.562 L2134.9 207.636 Q2134.9 210.807 2135.76 211.71 Q2136.64 212.613 2139.3 212.613 L2143.67 212.613 L2143.67 216.178 L2139.3 216.178 Q2134.37 216.178 2132.49 214.349 Q2130.62 212.497 2130.62 207.636 L2130.62 193.562 L2127.49 193.562 L2127.49 190.252 L2130.62 190.252 L2130.62 182.891 L2134.9 182.891 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2171.45 202.15 L2171.45 204.233 L2151.87 204.233 Q2152.15 208.631 2154.51 210.946 Q2156.89 213.238 2161.13 213.238 Q2163.58 213.238 2165.87 212.636 Q2168.19 212.034 2170.46 210.83 L2170.46 214.858 Q2168.17 215.83 2165.76 216.34 Q2163.35 216.849 2160.87 216.849 Q2154.67 216.849 2151.04 213.238 Q2147.42 209.627 2147.42 203.469 Q2147.42 197.104 2150.85 193.377 Q2154.3 189.627 2160.13 189.627 Q2165.36 189.627 2168.4 193.006 Q2171.45 196.363 2171.45 202.15 M2167.19 200.9 Q2167.15 197.405 2165.23 195.321 Q2163.33 193.238 2160.18 193.238 Q2156.61 193.238 2154.46 195.252 Q2152.33 197.266 2152.01 200.923 L2167.19 200.9 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2194.97 191.016 L2194.97 195.043 Q2193.17 194.117 2191.22 193.655 Q2189.28 193.192 2187.19 193.192 Q2184.02 193.192 2182.42 194.164 Q2180.85 195.136 2180.85 197.08 Q2180.85 198.562 2181.98 199.418 Q2183.12 200.252 2186.54 201.016 L2188 201.34 Q2192.54 202.312 2194.44 204.094 Q2196.36 205.854 2196.36 209.025 Q2196.36 212.636 2193.49 214.742 Q2190.64 216.849 2185.64 216.849 Q2183.56 216.849 2181.29 216.432 Q2179.04 216.039 2176.54 215.228 L2176.54 210.83 Q2178.91 212.057 2181.2 212.682 Q2183.49 213.284 2185.73 213.284 Q2188.74 213.284 2190.36 212.266 Q2191.98 211.224 2191.98 209.349 Q2191.98 207.613 2190.8 206.687 Q2189.65 205.761 2185.69 204.904 L2184.21 204.557 Q2180.25 203.724 2178.49 202.011 Q2176.73 200.275 2176.73 197.266 Q2176.73 193.608 2179.32 191.618 Q2181.92 189.627 2186.68 189.627 Q2189.04 189.627 2191.13 189.974 Q2193.21 190.321 2194.97 191.016 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip250)\" d=\"M2207.35 182.891 L2207.35 190.252 L2216.13 190.252 L2216.13 193.562 L2207.35 193.562 L2207.35 207.636 Q2207.35 210.807 2208.21 211.71 Q2209.09 212.613 2211.75 212.613 L2216.13 212.613 L2216.13 216.178 L2211.75 216.178 Q2206.82 216.178 2204.95 214.349 Q2203.07 212.497 2203.07 207.636 L2203.07 193.562 L2199.95 193.562 L2199.95 190.252 L2203.07 190.252 L2203.07 182.891 L2207.35 182.891 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=[]\n",
    "push!(y,accuracy_train)\n",
    "push!(y,accuracy_test)\n",
    "Plots.plot(update,y,label=[\"train\" \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-personality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
